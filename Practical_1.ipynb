{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKAJiNwUvqZM"
   },
   "source": [
    "# Introduction\n",
    "Welcome to the first practical for Graph Representation Learning. In this practical, we will be covering the content of the lectures about TransE.\n",
    "\n",
    "We will be using [PyTorch](https://pytorch.org/docs/stable/index.html) to implement TransE from scratch, building it up piece by piece.\n",
    "\n",
    "The main goal of this practical is to create a working implementation of TransE. There are also two optional parts: *filtered negative sampling* and *RotatE*.\n",
    "\n",
    "The notebook is divided into sections, each of which comes with complete or partially completed code. Before each snippet of code there will be a description of what we are about to implement. The sections of code you need to complete are marked as **Tasks**. The majority of the length of this practical comes from code already written for you, so don't panic at the apparent length! There are only 8 tasks for you to complete.\n",
    "\n",
    "Please ensure that you operate within the framework given in the notebook and bring any questions you may have to the practical demonstrators. We suggest that you **DO NOT** edit code that is a part of the framework, since this will make it more difficult for demonstrators to assist if your code is broken.\n",
    "\n",
    "Since we are working in a Jupyter Notebook, the code is very interactive. When you're stuck on something, try adding a new block of code below what you're working on and using it to debug your code. If you are new to Jupyter Notebooks, see [here](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow) for a brief introduction video. If you are using Google Colab (which we recommend doing), please ensure you have changed the runtime type to use a GPU, as it will make your code run much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5IOc-BMFfQw"
   },
   "source": [
    "# Imports\n",
    "\n",
    "Run the following blocks of code to install and import and the necessary python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kODAA4k06_PS"
   },
   "outputs": [],
   "source": [
    "# This code cell is commented out as it is not working properly locally for me.\n",
    "# %%capture\n",
    "# !pip install pykeen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HN8bYYUf7FQw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jupit\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pykeen\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils import data as torch_data\n",
    "from sklearn.metrics import average_precision_score\n",
    "from pykeen.datasets import Nations\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJmiuJqw78RR"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySL6dn4NBQQe"
   },
   "source": [
    "## Loading Nations from `pykeen`\n",
    "We will use `pykeen` to load the `Nations` dataset, which is a small knowledge graph with 14 entities, 55 relations, and 1992 triples describing countries and their political relationships.\n",
    "\n",
    "We first define a function to convert the `pykeen` datasets to lists of triples. We then create 3 lists of triples: one for training, one for validation, and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pr3eij-8Ejyg"
   },
   "outputs": [],
   "source": [
    "def create_triples_from_pykeen_dataset(dataset: pykeen.triples.triples_factory.TriplesFactory):\n",
    "    slcwa_instances=dataset.create_slcwa_instances(\n",
    "        batch_size=1,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    positive_dataset = [tuple(batch.positives[0].tolist()) for batch in slcwa_instances]\n",
    "    return positive_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PZfQ66zYEa71"
   },
   "outputs": [],
   "source": [
    "dataset = Nations()\n",
    "train_triples = create_triples_from_pykeen_dataset(dataset.training)\n",
    "valid_triples = create_triples_from_pykeen_dataset(dataset.validation)\n",
    "test_triples = create_triples_from_pykeen_dataset(dataset.testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "(12, 45, 5)\n",
      "1592\n",
      "199\n",
      "201\n"
     ]
    }
   ],
   "source": [
    "# My code cell\n",
    "print(type(train_triples))\n",
    "print(train_triples[0])\n",
    "print(len(train_triples)) # 1592 + 199 + 201 = 1992\n",
    "print(len(valid_triples))\n",
    "print(len(test_triples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtqqZDPjzRXH"
   },
   "source": [
    "#### Task 1\n",
    "Define a function `id_triple_as_labels` that takes in a triple of entity/relation ids and returns a triple of labels.\n",
    "\n",
    "*Hint: use the dictionaries `id2entity` and `id2relation` provided below*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qrHfuTC7o4AP"
   },
   "outputs": [],
   "source": [
    "id2entity = dataset.training.entity_id_to_label\n",
    "id2relation = dataset.training.relation_id_to_label\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def id_triple_as_labels(triple: Tuple):\n",
    "    head, relation, tail = triple\n",
    "    return id2entity[head], id2relation[relation], id2entity[tail]\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76Jeu4x-z8Ti"
   },
   "source": [
    "We can then run the following cell to see some of the facts from the **Nations** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xUcWeK4fo-U8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 45, 5)\n",
      "('usa', 'timesinceally', 'india')\n",
      "(11, 17, 12)\n",
      "('uk', 'exportbooks', 'usa')\n",
      "(4, 19, 8)\n",
      "('egypt', 'independence', 'jordan')\n",
      "(5, 45, 12)\n",
      "('india', 'timesinceally', 'usa')\n",
      "(1, 14, 11)\n",
      "('burma', 'embassy', 'uk')\n",
      "(11, 7, 5)\n",
      "('uk', 'commonbloc1', 'india')\n",
      "(1, 8, 6)\n",
      "('burma', 'commonbloc2', 'indonesia')\n",
      "(12, 17, 0)\n",
      "('usa', 'exportbooks', 'brazil')\n",
      "(0, 20, 5)\n",
      "('brazil', 'intergovorgs', 'india')\n",
      "(13, 28, 9)\n",
      "('ussr', 'ngoorgs3', 'netherlands')\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(train_triples[i])\n",
    "    print(id_triple_as_labels(train_triples[i]))\n",
    "# Note that, for instnace, we can know that 'uk' has id 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSS5FB7K9-c-"
   },
   "source": [
    "## Negative Sampling\n",
    "Simply training on positive facts will not suffice, since the model will then learn to just maximise the similarity measure for every possible fact in the database. Thus, for each positive fact, we need to sample a set of corrupted facts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTbungL01h45"
   },
   "source": [
    "#### Task 2\n",
    "First, define a function `get_corrupted_entities` that takes in a single positive sample and returns an array of corrupted entities. The positive sample is a tuple of integers, representing the IDs of the entities/relation.\n",
    "\n",
    "The function should also take in `train_dataset`, as you will need to access `train_dataset.negative_sample_size` (the number of corrupted entities to sample), `train_dataset.nentity` (the number of entities in the knowledge graph), and possibly `train_dataset.true_head` / `train_dataset.true_tail` (dictionaries providing the set of all known true triples for the given head / tail). The keys for the train_dataset.true_head dictionary are tuples of the form (relation, tail), and the keys for train_dataset.true_tail are tuples of the form (head, relation). The full definition of `TrainDataset` can be found further below if you need to refer to it.\n",
    "\n",
    "The function should also take in `mode`, which, if set to `mode == 'head'`, indicates that the head entity should be corrupted, and likewise the tail entity if `mode == 'tail'`. Your output should be a numpy array with shape `[train_dataset.negative_sample_size]`.\n",
    "\n",
    "Note: as an optional extra, you can sample corrupted entities such that the resulting triples are not known to be true in the knowledge graph. If you get stuck on this, first implement a simpler solution first and then come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<class 'NoneType'>\n",
      "[]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# My code cell\n",
    "# Check dictionary get() method\n",
    "dict_1 = {\n",
    "    'car' : 14,\n",
    "    'love' : 21,\n",
    "    'student' : 7\n",
    "}\n",
    "print(dict_1.get('csr'))\n",
    "print(type(dict_1.get('csr')))\n",
    "print(dict_1.get('csr',[]))\n",
    "print(type(dict_1.get('csr',[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "{1, 2, 3}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# My code cell\n",
    "# Check list and set\n",
    "set_1 = {1,2,2,3}\n",
    "print(type(set_1))\n",
    "set_2 = set([1,2,2,3])\n",
    "print(set_2)\n",
    "# set_3 = set(dict_1.get('csr')) # reason we need to use set(true_head.get((relation, tail), []))\n",
    "# print(set_3)\n",
    "set_4 = {}\n",
    "print(type(set_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 8]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My code cell\n",
    "# Check random.choices() method\n",
    "list_1 = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "random.choices(list_1, k = 3) # allow repeated sampling\n",
    "# May cause problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def get_corrupted_entities(positive_sample: Tuple, train_dataset: TrainDataset, mode: str) -> np.ndarray:\n",
    "    head, relation, tail = positive_sample\n",
    "    entities = list(range(train_dataset.nentity))  # Entities should be indexed from 0 to nentity - 1\n",
    "    negative_sample = []\n",
    "\n",
    "    # Access the true_head and true_tail dictionaries\n",
    "    true_head, true_tail = train_dataset.true_head, train_dataset.true_tail\n",
    "\n",
    "    if mode == 'head':\n",
    "        # Get true head entities for this (relation, tail) pair, or an empty set if not in true_head\n",
    "        true_head_entities = set(true_head.get((relation, tail), [])) # true_head.get((relation, tail)) is a numpy array\n",
    "        # Filter entities not in true head set for efficient sampling\n",
    "        possible_corruptions = [e for e in entities if e not in true_head_entities]\n",
    "        negative_sample = random.choices(possible_corruptions, k=train_dataset.negative_sample_size)\n",
    "\n",
    "    elif mode == \"tail\":\n",
    "        # Get true tail entities for this (head, relation) pair, or an empty set if not in true_tail\n",
    "        true_tail_entities = set(true_tail.get((head, relation), []))\n",
    "        # Filter entities not in true tail set for efficient sampling\n",
    "        possible_corruptions = [e for e in entities if e not in true_tail_entities]\n",
    "        negative_sample = random.choices(possible_corruptions, k=train_dataset.negative_sample_size)\n",
    "\n",
    "    return np.array(negative_sample)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKlY_jD_5SLE"
   },
   "source": [
    "#### Task 3\n",
    "Next, we can create the negative samples using the corrupted entities. Define a function `get_negative_sample` which takes in a positive sample and corrupted head/tail entities, which will be created by the `get_corrupted_entities` function we defined above (do not call to the above function, the corrupted entities will be passed in as arguments).\n",
    "\n",
    "The argument `positive_sample` is a tuple `(head, relation, tail)`. The arguments `corrupted_head_entities` and `corrupted_tail_entities` should have shape `[negative_sample_size]` each.\n",
    "\n",
    "Your function should return a `numpy` array with shape `[2*negative_sample_size, 3]`, by combining the corrupted entities with the positive sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fDbo_35aRC1G"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# corrupted_head_entities is the output of get_corrupted_entities fucntion above\n",
    "def get_negative_sample(positive_sample, corrupted_head_entities, corrupted_tail_entities):\n",
    "    corrupted_head_samples = np.zeros((corrupted_head_entities.shape[0], 3))\n",
    "    corrupted_head_samples[:, 0] = corrupted_head_entities\n",
    "    corrupted_head_samples[:, 1:3] = positive_sample[1:3]\n",
    "\n",
    "    corrupted_tail_samples = np.zeros((corrupted_tail_entities.shape[0], 3))\n",
    "    corrupted_tail_samples[:, 2] = corrupted_tail_entities\n",
    "    corrupted_tail_samples[:, 0:2] = positive_sample[0:2]\n",
    "\n",
    "    return np.concatenate((corrupted_head_samples, corrupted_tail_samples), axis=0)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st_ZunLh-yXD"
   },
   "source": [
    "## Train Dataset\n",
    "Now that we have written the functions we need to perform our negative sampling, let's combine everything together to create our `torch` training dataset.\n",
    "\n",
    "Notice that in the `__get_item__` function, we convert the samples to `torch` tensors before we return them. Up to this point, we have been working with `numpy` arrays; `torch` tensors have the same structure, but are optimised to run on the `GPU` and can also track gradients to be used for optimising parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5YPVX9Y_7h-g"
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, triples, nentity, nrelation, negative_sample_size):\n",
    "        self.len = len(triples)\n",
    "        self.triples = triples # all training triples\n",
    "        self.triple_set = set(triples) # unique triples\n",
    "        self.nentity = nentity # number of entities in the knowledge graph\n",
    "        self.nrelation = nrelation # number of relations in the knowledge graph\n",
    "        self.negative_sample_size = negative_sample_size // 2 # Half from heads, half from tails\n",
    "\n",
    "        # known triples for the given heads / tails\n",
    "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        Get an item from the Dataset.\n",
    "        '''\n",
    "        # Fetch a positive sample\n",
    "        positive_sample = self.triples[idx]\n",
    "\n",
    "        # Sample corrupted head and tail entities\n",
    "        corrupted_head_entities = get_corrupted_entities(positive_sample, self, 'head')\n",
    "        corrupted_tail_entities = get_corrupted_entities(positive_sample, self, 'tail')\n",
    "\n",
    "        # Create the negative sample\n",
    "        negative_sample = get_negative_sample(positive_sample, corrupted_head_entities, corrupted_tail_entities)\n",
    "\n",
    "        # Convert samples to torch tensors\n",
    "        negative_sample = torch.LongTensor(negative_sample)\n",
    "        positive_sample = torch.LongTensor(positive_sample)\n",
    "\n",
    "        return positive_sample, negative_sample\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        return positive_sample, negative_sample\n",
    "\n",
    "    @staticmethod\n",
    "    def get_true_head_and_tail(triples):\n",
    "        '''\n",
    "        Build a dictionary of true triples that will\n",
    "        be used to filter these true triples for negative sampling\n",
    "        '''\n",
    "\n",
    "        true_head = {}\n",
    "        true_tail = {}\n",
    "\n",
    "        for head, relation, tail in triples:\n",
    "            if (head, relation) not in true_tail:\n",
    "                true_tail[(head, relation)] = []\n",
    "            true_tail[(head, relation)].append(tail)\n",
    "            if (relation, tail) not in true_head:\n",
    "                true_head[(relation, tail)] = []\n",
    "            true_head[(relation, tail)].append(head)\n",
    "\n",
    "        for relation, tail in true_head:\n",
    "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
    "        for head, relation in true_tail:\n",
    "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))\n",
    "\n",
    "        return true_head, true_tail\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8pygs4q-WQo"
   },
   "source": [
    "## Test Dataset\n",
    "We will use a seperate dataset for our testing, with the batch size always set to 1. The test dataset will have two modes: `'head-batch'` and `'tail-batch'`. In the first mode, the dataset should return a positive sample and a list of all possible heads, and similarly for the second mode.\n",
    "\n",
    "Since we are doing filtered evaluation, we do not want triples which are known to be true to affect the ranking. Thus, we will filter the heads / tails out of our samples that yield triples which are known to be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_GCF-QCko82"
   },
   "source": [
    "#### Task 4\n",
    "Write a function `get_filtered_test_sample` that takes in a positive triple (`head, relation, tail`) and the `test_dataset` (which can be found further below). The function should return a list of size `test_dataset.nentity`, where each element is the ID of an entity. It is important for the downstream ranking that we ensure the list is this size.\n",
    "\n",
    "To filter the sample, if for some entity `x`, `(x, relation, tail)` already appears in the set of known triples, instead replace it with `head`. The set of known triples can be accessed by `test_dataset.triple_set`. You will need to check the value of `test_dataset.mode` and return a list of entities accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "def get_filtered_test_sample(head, relation, tail, test_dataset: Dataset) -> List:\n",
    "    filtered_sample = list(range(test_dataset.nentity))\n",
    "    \n",
    "    if test_dataset.mode == 'head-batch':\n",
    "        # Replace entities that form known triples in 'head-batch' mode\n",
    "        for i in range(test_dataset.nentity):\n",
    "            if (i, relation, tail) in test_dataset.triple_set:\n",
    "                filtered_sample[i] = head  # Replace with head if the triple is known\n",
    "\n",
    "    elif test_dataset.mode == 'tail-batch':\n",
    "        # Replace entities that form known triples in 'tail-batch' mode\n",
    "        for i in range(test_dataset.nentity):\n",
    "            if (head, relation, i) in test_dataset.triple_set:\n",
    "                filtered_sample[i] = tail  # Replace with tail if the triple is known\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Negative batch mode {test_dataset.mode} not supported')\n",
    "    return filtered_sample\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOV-9ixToFIK"
   },
   "source": [
    "Now we can use our filtered test sampling function to define our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6iXpggcLKjZ6"
   },
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, triples, all_true_triples, nentity, nrelation, mode):\n",
    "        self.len = len(triples)\n",
    "        self.triple_set = set(all_true_triples) # set of all known true triples\n",
    "        self.triples = triples # test triples\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.mode = mode # 'head-batch' or 'tail-batch'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        head, relation, tail = self.triples[idx] # fetch a positive sample from the test triples\n",
    "\n",
    "        # get the filtered sample using the function we defined\n",
    "        filtered_sample = get_filtered_test_sample(head, relation, tail, self)\n",
    "\n",
    "        # convert to torch tensors\n",
    "        filtered_sample = torch.LongTensor(filtered_sample)\n",
    "        positive_sample = torch.LongTensor((head, relation, tail))\n",
    "\n",
    "        return positive_sample, filtered_sample, self.mode\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(data):\n",
    "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
    "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
    "        mode = data[0][2]\n",
    "        return positive_sample, negative_sample, mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NkNwwu5W-qj2"
   },
   "source": [
    "## Dataset Iterator\n",
    "As a final step towards constructing our datasets, we define a class that allows us to convert `torch` dataloaders into python iterators, which will make it simpler for us to define training and testing step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XaQf4pHj-mCg"
   },
   "outputs": [],
   "source": [
    "class OneShotIterator:\n",
    "    def __init__(self, dataloader):\n",
    "        self.iterator = self.one_shot_iterator(dataloader)\n",
    "\n",
    "    def __next__(self):\n",
    "        return next(self.iterator)\n",
    "\n",
    "    @staticmethod\n",
    "    def one_shot_iterator(dataloader):\n",
    "        '''\n",
    "        Transform a PyTorch Dataloader into python iterator\n",
    "        '''\n",
    "        while True:\n",
    "            for data in dataloader:\n",
    "                yield data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3L96bkppgXn"
   },
   "source": [
    "We will create the actual train and test datasets in code further below, but here follows some code for constructing them, in case you would like to use them for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tJmSoj1DpoFy"
   },
   "outputs": [],
   "source": [
    "# make the train dataloader\n",
    "train_dataloader = DataLoader(\n",
    "    TrainDataset(train_triples,\n",
    "                 14, # nentity\n",
    "                 55, # nrelation\n",
    "                 128), # negative sampling size\n",
    "    batch_size=500,\n",
    "    shuffle=True,\n",
    "    # num_workers=1, original code\n",
    "    num_workers=0,\n",
    "    collate_fn=TrainDataset.collate_fn\n",
    ")\n",
    "# convert to an iterator\n",
    "train_iterator = OneShotIterator(train_dataloader)\n",
    "# get a sample from the train iterator\n",
    "positive_sample, negative_sample = next(train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NJXmueor5HB"
   },
   "source": [
    "If your code is correct, the following output should be `torch.Size([500, 128, 3])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z2UO0M1YqlhC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 128, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "thhn9ZCOqj_7"
   },
   "outputs": [],
   "source": [
    "# make the test dataloader\n",
    "known_true_triples = train_triples + valid_triples + test_triples\n",
    "test_dataloader_head = DataLoader(\n",
    "    TestDataset(\n",
    "        test_triples,\n",
    "        known_true_triples,\n",
    "        14, # nentity\n",
    "        55, # nrelation\n",
    "        'head-batch'\n",
    "    ),\n",
    "    batch_size=1,\n",
    "    # num_workers=1,\n",
    "    num_workers=0,\n",
    "    collate_fn=TestDataset.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_sbcRuXryAr"
   },
   "source": [
    "If your code is correct, the following output should be `torch.Size([1, 14])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "jl-ZEaM7rDaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "for positive_sample, negative_sample, mode in test_dataloader_head:\n",
    "    print(negative_sample.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMHxh01176Aw"
   },
   "source": [
    "# Model\n",
    "We now define our model, `KGEModel`. It is built in such a way that we can implement different dissimilarity measures within it.\n",
    "\n",
    "From here onwards, we will be working with `torch` tensors instead of `numpy` arrays, so make sure you are using `torch` operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghEH_cnNDMpQ"
   },
   "source": [
    "## Parameter Initialisation\n",
    "We will use `torch.nn.Parameter` to store our embeddings for entities and relations. We define a function `init_params` which initialises an embedding tensor of the given size and randomly samples values from the uniform distribution `[-embedding_range, embedding_range]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZlTLS7IhDL8u"
   },
   "outputs": [],
   "source": [
    "def init_params(tensor_size: tuple, embedding_range: float) -> nn.Parameter:\n",
    "    embedding = nn.Parameter(torch.zeros(tensor_size))\n",
    "    nn.init.uniform_(\n",
    "        tensor=embedding,\n",
    "        a=-embedding_range,\n",
    "        b=embedding_range\n",
    "    )\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRfGJezKpO6Z"
   },
   "source": [
    "## Scoring Function\n",
    "Different KG embedding models use different dissimilarity measures (aka scoring functions). We will define one for **TransE** and optionally define one for **RotatE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUV1Q2CItxlD"
   },
   "source": [
    "#### Task 5\n",
    "Define a scoring function for **TransE** that takes in the head, relation, and tail, and returns a score for the triple. Each argument tensor has size `[batch_size, embedding_size]`. You may use either the $l_1$ or $l_2$ norm. Your output tensor should have size `[batch_size]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "POsujQKbpUzx"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# head, relation, tail are of shape [batch_size, embedding_size]\n",
    "def TransE(head : torch.tensor, relation : torch.tensor, tail : torch.tensor, norm_type=1):\n",
    "    translation = head + relation - tail\n",
    "    if norm_type == 1:\n",
    "        score = torch.norm(translation, p=1, dim=1)  # l1 norm\n",
    "    elif norm_type == 2:\n",
    "        score = torch.norm(translation, p=2, dim=1)  # l2 norm\n",
    "    else:\n",
    "        raise ValueError(\"norm_type must be either 1 (l1) or 2 (l2)\")\n",
    "    # score is of shape [batch_size]\n",
    "    # the higher the score is, the poor the performance of the model and rank increases\n",
    "    return score\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZgxQhEepZ8m"
   },
   "source": [
    "### Task 6 (Optional)\n",
    "This is an optional task. You should get **TransE** working completely first and then come back to this.\n",
    "\n",
    "Define a scoring function for **RotatE**. The head and tail will have size `[batch_size, 2 * embedding_size]` to store both the real and imaginary parts of the entities. *Hint: you can use `torch.chunk()` to split the tensor into its real and imaginary components*.\n",
    "\n",
    "The relation will have size `[batch_size, embedding_size]`, representing the phase $\\theta$ of the relation. *Hint: The real and imaginary components of the relation can be computed with `torch.cos` and `torch.sin`*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Euler's Formula for Rotation\n",
    "Let us write down the formula for rotating the entities explicitly.\n",
    "We know **head entity** $\\mathbf{h}$ has the following complex embedding:\n",
    "$\n",
    "\\mathbf{h}=\\mathbf{h}_{\\text {real }}+i \\cdot \\mathbf{h}_{\\text {imag }}\n",
    "$\n",
    "and each element $r_i$ of the relation embedding $\\mathbf{r}$ is of the form $\\theta_{r,i}$ (but in the slides, the entries are given by $e^{i \\theta_{r,i}}$ with $|r_i| = 1$).\n",
    "\n",
    "According to Euler's formula, a complex number $z=$ $a+i \\cdot b$ can be rotated by an angle $\\theta$ by multiplying it by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& e^{i \\theta}=\\cos (\\theta)+i \\sin (\\theta) \\\\\n",
    "& \\quad z^{\\prime}=z \\cdot e^{i \\theta}=(a+i \\cdot b)(\\cos (\\theta)+i \\cdot \\sin (\\theta))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "When we apply this rotation to each dimension $i$ of $h$ by $\\theta_{r, i}$, we get:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& h_{\\mathrm{real}, i}^{\\prime}=h_{\\mathrm{real}, i} \\cdot \\cos \\left(\\theta_{r, i}\\right)-h_{\\mathrm{imag}, i} \\cdot \\sin \\left(\\theta_{r, i}\\right) \\\\\n",
    "& h_{\\mathrm{imag}, i}^{\\prime}=h_{\\mathrm{real}, i} \\cdot \\sin \\left(\\theta_{r, i}\\right)+h_{\\mathrm{imag}, i} \\cdot \\cos \\left(\\theta_{r, i}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "This produces the rotated real and imaginary components of $h$ for each dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3])\n",
      "torch.Size([4])\n",
      "<class 'tuple'>\n",
      "tensor([[ 0.3792, -0.8419, -0.7589, -0.1385],\n",
      "        [ 0.5661, -0.4204, -0.6449,  0.3077]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.3792, -0.8419, -0.7589, -0.1385]]),\n",
       " tensor([[ 0.5661, -0.4204, -0.6449,  0.3077]]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My code cell for checking `torch.chunk()` function\n",
    "tensor_2 = torch.arange(4)\n",
    "print(tensor_2)\n",
    "print(tensor_2.size())\n",
    "print(type(tensor_2.chunk(2)))\n",
    "tensor_2.chunk(2)[0]\n",
    "tensor_3 = torch.randn(2,4)\n",
    "print(tensor_3)\n",
    "tensor_3.chunk(2, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "T_OU9yU_pbm2"
   },
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "def RotatE(head, relation, tail):\n",
    "    # Split the head and tail into real and imaginary parts along the embedding size (last) dimension\n",
    "    head_real, head_imag = torch.chunk(head, 2, dim=-1)\n",
    "    tail_real, tail_imag = torch.chunk(tail, 2, dim=-1)\n",
    "\n",
    "    # Compute real and imaginary parts of the rotation\n",
    "    relation_real = torch.cos(relation) # of shape [batch_size]\n",
    "    relation_imag = torch.sin(relation)\n",
    "\n",
    "    # Rotate head\n",
    "    rotated_head_real = head_real * relation_real - head_imag * relation_imag\n",
    "    rotated_head_imag = head_real * relation_imag + head_imag * relation_real\n",
    "    \n",
    "    # Calculate the difference (dissimilarity) between rotated head and tail\n",
    "    score_real = rotated_head_real - tail_real  # of shape [batch_size]\n",
    "    score_imag = rotated_head_imag - tail_imag\n",
    "    \n",
    "    # Combine real and imaginary parts to get final score\n",
    "    score = torch.norm(torch.cat([score_real, score_imag], dim=-1), p=2, dim=-1)\n",
    "\n",
    "    return score\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yFFX3RsxpPaI"
   },
   "source": [
    "## Full Model Definition\n",
    "Now we can use our scoring function to define the model. Notice that in the `forward` function, we use `torch.index_select` to fetch the entity / relation embeddings from the their indices. `sample` is a tensor with the size `[batch_size, 3]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "gAfa7_U77o5L"
   },
   "outputs": [],
   "source": [
    "class KGEModel(nn.Module):\n",
    "    def __init__(self, model_name: str, nentity: int, nrelation: int, hidden_dim: int, gamma: float,\n",
    "                 double_entity_embedding: bool=False, double_relation_embedding: bool=False):\n",
    "        super(KGEModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.nentity = nentity\n",
    "        self.nrelation = nrelation\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epsilon = 2.0\n",
    "\n",
    "        self.gamma = nn.Parameter(\n",
    "            torch.Tensor([gamma]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.embedding_range = nn.Parameter(\n",
    "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),\n",
    "            requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.entity_dim = 2*hidden_dim if double_entity_embedding else hidden_dim\n",
    "        self.relation_dim = 2*hidden_dim if double_relation_embedding else hidden_dim\n",
    "\n",
    "        # Create entity and relation embeddings\n",
    "        self.entity_embedding = init_params(tensor_size=(nentity, self.entity_dim),\n",
    "                                             embedding_range=self.embedding_range.item())\n",
    "        self.relation_embedding = init_params(tensor_size=(nrelation, self.relation_dim),\n",
    "                                              embedding_range=self.embedding_range.item())\n",
    "\n",
    "        # This code supports easily adding new models like RotatE and ComplEx\n",
    "        # Do not forget to modify this line when you add a new model in the \"forward\" function\n",
    "        if model_name not in ['TransE', 'RotatE']:\n",
    "            raise ValueError('model %s not supported' % model_name)\n",
    "\n",
    "        if model_name == 'RotatE' and not double_entity_embedding:\n",
    "            raise ValueError('RotatE should use --double_entity_embedding')\n",
    "        if model_name == 'TransE' and double_entity_embedding:\n",
    "            raise ValueError('TransE should not use --double_entity_embedding')\n",
    "\n",
    "    def forward(self, sample):\n",
    "        '''\n",
    "        Forward function that calculate the score of a batch of triples.\n",
    "        Sample is a batch of triples.\n",
    "        '''\n",
    "        head = torch.index_select(\n",
    "            self.entity_embedding,\n",
    "            dim=0,\n",
    "            index=sample[:,0]\n",
    "        )\n",
    "\n",
    "        relation = torch.index_select(\n",
    "            self.relation_embedding,\n",
    "            dim=0,\n",
    "            index=sample[:,1]\n",
    "        )\n",
    "\n",
    "        tail = torch.index_select(\n",
    "            self.entity_embedding,\n",
    "            dim=0,\n",
    "            index=sample[:,2]\n",
    "        )\n",
    "\n",
    "        # Other models can be added here\n",
    "        dissimilarity_measure = {\n",
    "            'TransE': TransE,\n",
    "            'RotatE': RotatE,\n",
    "        }\n",
    "\n",
    "        if self.model_name in dissimilarity_measure:\n",
    "            score = dissimilarity_measure[self.model_name](head, relation, tail)\n",
    "        else:\n",
    "            raise ValueError('model %s not supported' % self.model_name)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLu2TKQh2Su_"
   },
   "source": [
    "# Training\n",
    "In this section, we will first write a function to compute the model loss given a set of positive and negative samples, and then use it to define a single training step for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0B2CufDyMxE"
   },
   "source": [
    "#### Task 7\n",
    "Before we can define a training step for our model, we must define a loss function for the model. We use negative sampling loss (from the **RotatE** paper), but without the self-adversarial parameter. Remember to refer to the lecture slides if you get stuck on this.\n",
    "\n",
    "Define a function `get_model_loss` which takes in the `KGEModel`, the positive sample, and the negative sample, and returns a tuple of the loss, the positive sample loss, and the negative sample loss.\n",
    "\n",
    "The positive sample will have size `[batch_size, 3]`, the negative sample will have size `[batch_size * negative_sampling_size, 3]`, and the margin $\\gamma$ can be accessed through `model.gamma`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]], dtype=torch.int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My code block for testing tensor size\n",
    "tensor_1 = torch.zeros([2, 4], dtype=torch.int32)\n",
    "print(tensor_1)\n",
    "tensor_1.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the negative sampling loss without the self-adversarial parameter is given by the following:\n",
    "$$\n",
    "\\mathscr{L}=-\\log \\sigma(\\gamma-d(\\mathbf{h} \\odot \\mathbf{r}, \\mathbf{t}))-\\sum_{r\\left(h^{\\prime}, t^{\\prime}) \\in \\mu[r(h, t)]\\right.} \\frac{1}{k} \\log \\sigma\\left(d\\left(\\mathbf{h}^{\\prime} \\odot \\mathbf{r}, \\mathbf{t}^{\\prime}\\right)-\\gamma\\right),\n",
    "$$\n",
    "where $\\gamma$ is a margin, $\\sigma$ is the sigmoid function, and $\\mu[r(h, t)]$ is a set of $k$ negative samples for $r(h, t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# calculate the time needed\n",
    "from time import time\n",
    "\n",
    "def get_model_loss(model: KGEModel, positive_sample: torch.tensor, negative_sample: torch.tensor) -> Tuple:\n",
    "\n",
    "    '''\n",
    "    for every positive_sample, we have negative_sampling_size many negative_sample\n",
    "    positive_sample is of shape [batch_size, 3]\n",
    "    negative_sample is of shape [batch_size * negative_sampling_size, 3]\n",
    "    we use model.forward to compute the score\n",
    "    '''\n",
    "\n",
    "    loss = 0\n",
    "    sample_loss = 0\n",
    "    positive_sample_loss, negative_sample_loss = 0, 0\n",
    "    batch_size = positive_sample.size(0)\n",
    "\n",
    "    # Get the positive_sample_loss\n",
    "    # - log(sigmoid(y-d(h,t)))\n",
    "    positive_score = model(positive_sample) # of shape [batch_size]\n",
    "    positive_sample_loss = -torch.log(torch.sigmoid(model.gamma - positive_score)) # of shape [batch_size]\n",
    "\n",
    "    # Get the negative_sample_loss\n",
    "    negative_score = model(negative_sample) # of shape [batch_size * negative_sampling_size]\n",
    "    negative_sample_loss = -torch.log(torch.sigmoid(negative_score - model.gamma)) # of shape [batch_size * negative_sampling_size]\n",
    "    negative_sample_loss = negative_sample_loss.view(batch_size, -1)\n",
    "    negative_sample_loss = negative_sample_loss.mean(dim=1) # Now of shape [batch_size]\n",
    "\n",
    "\n",
    "    sample_loss = positive_sample_loss + negative_sample_loss\n",
    "    loss = sample_loss.mean()\n",
    "\n",
    "    return loss, positive_sample_loss.mean(), negative_sample_loss.mean()\n",
    "\n",
    "\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wFfnDat0I_e"
   },
   "source": [
    "We can now define a single train step for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "XMIYGukH2UnR"
   },
   "outputs": [],
   "source": [
    "def train_step(model, optimizer, train_iterator, args):\n",
    "    '''\n",
    "    A single train step. Apply back-propation and return the loss\n",
    "    '''\n",
    "\n",
    "    model.train() # tell the torch model it's about to be trained\n",
    "\n",
    "    optimizer.zero_grad() # explicitly set gradients to 0 before starting backprop\n",
    "\n",
    "    positive_sample, negative_sample = next(train_iterator) # fetch samples from the dataset\n",
    "\n",
    "    # reshape the negative sample\n",
    "    # it will now have shape [batch_size * negative_sampling_size, 3]\n",
    "    negative_sample = torch.reshape(negative_sample, (-1, 3))\n",
    "\n",
    "    # move tensors to GPU\n",
    "    if args.cuda:\n",
    "        positive_sample = positive_sample.cuda()\n",
    "        negative_sample = negative_sample.cuda()\n",
    "\n",
    "    # compute the loss\n",
    "    loss, positive_sample_loss, negative_sample_loss = get_model_loss(model, positive_sample, negative_sample)\n",
    "\n",
    "    # apply loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    log = {\n",
    "        'positive_sample_loss': positive_sample_loss.item(),\n",
    "        'negative_sample_loss': negative_sample_loss.item(),\n",
    "        'loss': loss.item()\n",
    "    }\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWCLuazd2EZ7"
   },
   "source": [
    "# Testing\n",
    "In this section, we will first write a function to get the ranking of a positive entity compared to its corrupted counterparts, and then use that to define a single test step for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSsRpPyJ1V3u"
   },
   "source": [
    "#### Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkDrEhHl3ES7"
   },
   "source": [
    "Define a function `get_ranking` which takes in entity scores and the index of the positive entity. The function should return an integer representing the rank of the score of the positive entity in relation to the other entities. Note that a rank of 1 represents having the *lowest* score.\n",
    "\n",
    "`entity_scores` has size `[nentity]`. Recall from the function we defined further above that we filtered out known true triples by replacing the corrupted heads with the actual head, so some of the entity scores may actually be that of the positive entity, even when they are not in the index of that entity.\n",
    "\n",
    "*Hint: torch.argsort will be very useful for this task*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# One can choose to 'break ties' in this ranking function by consider offsetting the score of positive entity by some small epsilon amount; but I didn't do this below.\n",
    "def get_ranking(entity_scores: torch.tensor, positive_entity: int) -> int:\n",
    "    sorted_indices = torch.argsort(entity_scores)\n",
    "    ranking = torch.where(sorted_indices == positive_entity)[0].item() + 1\n",
    "    return ranking\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmlKohB21Y10"
   },
   "source": [
    "We can now define a single test step for the model, using the ranking function to compute MRR, MR, and HITS@k metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "1muCky012KUM"
   },
   "outputs": [],
   "source": [
    "def test_step(model, test_triples, all_true_triples, args):\n",
    "    '''\n",
    "    Evaluate the model on test or valid datasets\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    #Prepare dataloader for evaluation\n",
    "    test_dataloader_head = DataLoader(\n",
    "        TestDataset(\n",
    "            test_triples,\n",
    "            all_true_triples,\n",
    "            args.nentity,\n",
    "            args.nrelation,\n",
    "            'head-batch'\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        # num_workers=max(1, args.cpu_num//2),\n",
    "        num_workers=0,\n",
    "        collate_fn=TestDataset.collate_fn\n",
    "    )\n",
    "\n",
    "    test_dataloader_tail = DataLoader(\n",
    "        TestDataset(\n",
    "            test_triples,\n",
    "            all_true_triples,\n",
    "            args.nentity,\n",
    "            args.nrelation,\n",
    "            'tail-batch'\n",
    "        ),\n",
    "        batch_size=args.test_batch_size,\n",
    "        # num_workers=max(1, args.cpu_num//2),\n",
    "        num_workers=0,\n",
    "        collate_fn=TestDataset.collate_fn\n",
    "    )\n",
    "\n",
    "    test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
    "\n",
    "    logs = []\n",
    "\n",
    "    step = 0\n",
    "    total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
    "\n",
    "    # torch.no_grad() since we don't need to track gradients when we're testing\n",
    "    with torch.no_grad():\n",
    "        for test_dataset in test_dataset_list: # each of head / tail\n",
    "            for positive_sample, negative_sample, mode in test_dataset:\n",
    "                # take a sample from the test dataset\n",
    "                if args.cuda:\n",
    "                    positive_sample = positive_sample.cuda()\n",
    "                    negative_sample = negative_sample.cuda()\n",
    "\n",
    "                batch_size = positive_sample.size(0)\n",
    "                assert batch_size == 1, 'evaluation batch size must be set to 1'\n",
    "\n",
    "                # build the negative sample from the entities\n",
    "                # currently, negative_sample is just a list of entities\n",
    "                # [1, 14, 3] for Nations\n",
    "                built_negative_sample = torch.zeros((batch_size, negative_sample.size()[1], 3), dtype=int)\n",
    "                if args.cuda:\n",
    "                    built_negative_sample = built_negative_sample.cuda()\n",
    "\n",
    "                if mode == 'head-batch':\n",
    "                    built_negative_sample[:, :, 0] = negative_sample\n",
    "                    built_negative_sample[:, :, 1:3] = positive_sample[:, 1:3].unsqueeze(dim=1).expand((-1, built_negative_sample.size(1), -1))\n",
    "                else:\n",
    "                    built_negative_sample[:, :, 2] = negative_sample\n",
    "                    built_negative_sample[:, :, 0:2] = positive_sample[:, 0:2].unsqueeze(dim=1).expand((-1, built_negative_sample.size(1), -1))\n",
    "\n",
    "                # get the scores for each entity\n",
    "                negative_sample = built_negative_sample.reshape((-1, 3))\n",
    "                entity_scores = model(negative_sample)\n",
    "\n",
    "                # retrieve the positive entity\n",
    "                if mode == 'head-batch':\n",
    "                    positive_entity = positive_sample[:, 0].item()\n",
    "                elif mode == 'tail-batch':\n",
    "                    positive_entity = positive_sample[:, 2].item()\n",
    "                else:\n",
    "                    raise ValueError('mode %s not supported' % mode)\n",
    "\n",
    "                # get the ranking of the positive entity\n",
    "                ranking = get_ranking(entity_scores, positive_entity)\n",
    "\n",
    "                # compute and append logs\n",
    "                logs.append({\n",
    "                    'MRR': 1.0/ranking,\n",
    "                    'MR': float(ranking),\n",
    "                    'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
    "                    'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
    "                    'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
    "                })\n",
    "\n",
    "                if step % args.test_log_steps == 0:\n",
    "                    print('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
    "\n",
    "                step += 1\n",
    "\n",
    "    metrics = {}\n",
    "    for metric in logs[0].keys():\n",
    "        metrics[metric] = sum([log[metric] for log in logs])/len(logs)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2y0CUaUWsepS"
   },
   "source": [
    "# Running\n",
    "To make the running of experiments easier, we will define several help functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJkg1N0skfAp"
   },
   "source": [
    "## Arguments\n",
    "`argparse` is a very useful library for managing program arguments, particulary when executing from the command line. Default arguments are defined here. If you want to change argument values, do not do it in this block of code, rather change them in the arguments that are passed through to the program (see further below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "V5szUw0_khK8"
   },
   "outputs": [],
   "source": [
    "def parse_args(args=None):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Training and Testing Knowledge Graph Embedding Models',\n",
    "        usage='train.py [<args>] [-h | --help]'\n",
    "    )\n",
    "\n",
    "    parser.add_argument('--cuda', action='store_true', help='use GPU')\n",
    "\n",
    "    parser.add_argument('--do_train', action='store_true')\n",
    "    parser.add_argument('--do_valid', action='store_true')\n",
    "    parser.add_argument('--do_test', action='store_true')\n",
    "    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data')\n",
    "\n",
    "    parser.add_argument('--model', default='TransE', type=str)\n",
    "    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n",
    "    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n",
    "\n",
    "    parser.add_argument('-n', '--negative_sample_size', default=128, type=int)\n",
    "    parser.add_argument('-d', '--hidden_dim', default=100, type=int, help='Embedding size')\n",
    "    parser.add_argument('-g', '--gamma', default=2.0, type=float, help='Fixed margin parameter')\n",
    "    parser.add_argument('-b', '--batch_size', default=1024, type=int)\n",
    "    parser.add_argument('--test_batch_size', default=1, type=int, help='valid/test batch size (must be 1)')\n",
    "\n",
    "    parser.add_argument('-lr', '--learning_rate', default=0.0001, type=float)\n",
    "    parser.add_argument('-cpu', '--cpu_num', default=1, type=int)\n",
    "    parser.add_argument('--max_steps', default=100, type=int)\n",
    "\n",
    "    parser.add_argument('--valid_steps', default=20, type=int, help='how often to check accuracy on validation dataset')\n",
    "    parser.add_argument('--log_steps', default=10, type=int, help='train log every xx steps')\n",
    "    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n",
    "\n",
    "    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n",
    "\n",
    "    parser.parse_args(args)\n",
    "    return parser.parse_args(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TccTbD_UklT7"
   },
   "source": [
    "## Logging\n",
    "The below function is used to help with logging metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_2a2v-Cpkmwu"
   },
   "outputs": [],
   "source": [
    "def log_metrics(mode, step, metrics):\n",
    "    '''\n",
    "    Print the evaluation logs\n",
    "    '''\n",
    "    for metric in metrics:\n",
    "        print('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQcVNKj38kol"
   },
   "source": [
    "## Main Program Loop\n",
    "We can finally bring everything we've done together into the main program loop. Please refer to the comments in the code to understand how it operates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "a_sON4K08mAV"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    if (not args.do_train) and (not args.do_valid) and (not args.do_test):\n",
    "        raise ValueError('one of train/val/test mode must be chosen.')\n",
    "\n",
    "    # use CUDA if possible\n",
    "    args.cuda = torch.cuda.is_available()\n",
    "\n",
    "    # print dataset parameters\n",
    "    entity2id = dataset.training.entity_to_id\n",
    "    relation2id = dataset.training.relation_to_id\n",
    "\n",
    "    nentity = len(entity2id)\n",
    "    nrelation = len(relation2id)\n",
    "\n",
    "    args.nentity = nentity\n",
    "    args.nrelation = nrelation\n",
    "\n",
    "    print('Model: %s' % args.model)\n",
    "    print('#entity: %d' % nentity)\n",
    "    print('#relation: %d' % nrelation)\n",
    "\n",
    "    print('#train: %d' % len(train_triples))\n",
    "    print('#valid: %d' % len(valid_triples))\n",
    "    print('#test: %d' % len(test_triples))\n",
    "\n",
    "    # all true triples\n",
    "    all_true_triples = train_triples + valid_triples + test_triples\n",
    "\n",
    "    # create the model\n",
    "    kge_model = KGEModel(\n",
    "        model_name=args.model,\n",
    "        nentity=nentity,\n",
    "        nrelation=nrelation,\n",
    "        hidden_dim=args.hidden_dim,\n",
    "        gamma=args.gamma,\n",
    "        double_entity_embedding=args.double_entity_embedding,\n",
    "        double_relation_embedding=args.double_relation_embedding\n",
    "    )\n",
    "\n",
    "    # output the model params\n",
    "    print('\\nModel Parameter Configuration:')\n",
    "    for name, param in kge_model.named_parameters():\n",
    "        print('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
    "\n",
    "    if args.cuda:\n",
    "        kge_model = kge_model.cuda()\n",
    "\n",
    "    if args.do_train:\n",
    "        # set training dataloader iterator\n",
    "        train_dataloader = DataLoader(\n",
    "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size),\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            # num_workers=max(1, args.cpu_num//2),\n",
    "            num_workers=0,\n",
    "            collate_fn=TrainDataset.collate_fn\n",
    "        )\n",
    "        train_iterator = OneShotIterator(train_dataloader)\n",
    "\n",
    "        # set training configuration\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
    "            lr=args.learning_rate\n",
    "        )\n",
    "\n",
    "    init_step = 1\n",
    "    step = init_step\n",
    "\n",
    "    # do an initial evaluation to see metrics for a random model\n",
    "    print('\\nEvaluating initial model on Valid Dataset...')\n",
    "    metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
    "    log_metrics('Valid', step, metrics)\n",
    "\n",
    "    # log training parameters\n",
    "    print('\\nStart Training...')\n",
    "    print('init_step = %d' % init_step)\n",
    "    print('batch_size = %d' % args.batch_size)\n",
    "    print('hidden_dim = %d' % args.hidden_dim)\n",
    "    print('gamma = %f' % args.gamma)\n",
    "\n",
    "    if args.do_train:\n",
    "        print('learning_rate = %f' % args.learning_rate)\n",
    "\n",
    "        training_logs = []\n",
    "\n",
    "        # training loop\n",
    "        for step in range(init_step, args.max_steps):\n",
    "            # perform a single train step\n",
    "            log = train_step(kge_model, optimizer, train_iterator, args)\n",
    "\n",
    "            # record the logs\n",
    "            training_logs.append(log)\n",
    "\n",
    "            # check if logs should be displayed\n",
    "            if step % args.log_steps == 0:\n",
    "                metrics = {}\n",
    "                for metric in training_logs[0].keys():\n",
    "                    metrics[metric] = sum([log[metric] for log in training_logs])/len(training_logs)\n",
    "                print('\\nTraining metrics...')\n",
    "                log_metrics('Training average', step, metrics)\n",
    "                training_logs = []\n",
    "\n",
    "            # check if metrics should be reported on the validation set\n",
    "            if args.do_valid and step % args.valid_steps == 0:\n",
    "                print('\\nEvaluating on Valid Dataset...')\n",
    "                metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
    "                log_metrics('Valid', step, metrics)\n",
    "\n",
    "    # compute final metrics after training is complete\n",
    "\n",
    "    if args.do_valid:\n",
    "        print('Evaluating on Valid Dataset...')\n",
    "        metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
    "        log_metrics('Valid', step, metrics)\n",
    "\n",
    "    if args.do_test:\n",
    "        print('Evaluating on Test Dataset...')\n",
    "        metrics = test_step(kge_model, test_triples, all_true_triples, args)\n",
    "        log_metrics('Test', step, metrics)\n",
    "\n",
    "    if args.evaluate_train:\n",
    "        print('Evaluating on Training Dataset...')\n",
    "        metrics = test_step(kge_model, train_triples, all_true_triples, args)\n",
    "        log_metrics('Test', step, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNOqo1cs6FcQ"
   },
   "source": [
    "The below code can be used to run the main program loop. Model arguments can be adjusted by changing / adding / removing the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "i-OAPHgf6Eqa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: TransE\n",
      "#entity: 14\n",
      "#relation: 55\n",
      "#train: 1592\n",
      "#valid: 199\n",
      "#test: 201\n",
      "\n",
      "Model Parameter Configuration:\n",
      "Parameter gamma: torch.Size([1]), require_grad = False\n",
      "Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "Parameter entity_embedding: torch.Size([14, 100]), require_grad = True\n",
      "Parameter relation_embedding: torch.Size([55, 100]), require_grad = True\n",
      "\n",
      "Evaluating initial model on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 1: 0.149247\n",
      "Valid MR at step 1: 8.258794\n",
      "Valid HITS@1 at step 1: 0.000000\n",
      "Valid HITS@3 at step 1: 0.075377\n",
      "Valid HITS@10 at step 1: 0.743719\n",
      "\n",
      "Start Training...\n",
      "init_step = 1\n",
      "batch_size = 1024\n",
      "hidden_dim = 100\n",
      "gamma = 2.000000\n",
      "learning_rate = 0.000100\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 10: 1.468488\n",
      "Training average negative_sample_loss at step 10: 0.329678\n",
      "Training average loss at step 10: 1.798167\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 20: 1.378018\n",
      "Training average negative_sample_loss at step 20: 0.356637\n",
      "Training average loss at step 20: 1.734655\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 20: 0.150740\n",
      "Valid MR at step 20: 8.208543\n",
      "Valid HITS@1 at step 20: 0.000000\n",
      "Valid HITS@3 at step 20: 0.085427\n",
      "Valid HITS@10 at step 20: 0.746231\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 30: 1.294745\n",
      "Training average negative_sample_loss at step 30: 0.383818\n",
      "Training average loss at step 30: 1.678563\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 40: 1.218399\n",
      "Training average negative_sample_loss at step 40: 0.411011\n",
      "Training average loss at step 40: 1.629410\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 40: 0.150343\n",
      "Valid MR at step 40: 8.241206\n",
      "Valid HITS@1 at step 40: 0.000000\n",
      "Valid HITS@3 at step 40: 0.082915\n",
      "Valid HITS@10 at step 40: 0.733668\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 50: 1.149220\n",
      "Training average negative_sample_loss at step 50: 0.438333\n",
      "Training average loss at step 50: 1.587553\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 60: 1.087842\n",
      "Training average negative_sample_loss at step 60: 0.463640\n",
      "Training average loss at step 60: 1.551482\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 60: 0.154895\n",
      "Valid MR at step 60: 8.087940\n",
      "Valid HITS@1 at step 60: 0.000000\n",
      "Valid HITS@3 at step 60: 0.090452\n",
      "Valid HITS@10 at step 60: 0.741206\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 70: 1.032207\n",
      "Training average negative_sample_loss at step 70: 0.487995\n",
      "Training average loss at step 70: 1.520202\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 80: 0.982738\n",
      "Training average negative_sample_loss at step 80: 0.510696\n",
      "Training average loss at step 80: 1.493434\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 80: 0.159596\n",
      "Valid MR at step 80: 7.894472\n",
      "Valid HITS@1 at step 80: 0.000000\n",
      "Valid HITS@3 at step 80: 0.092965\n",
      "Valid HITS@10 at step 80: 0.776382\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 90: 0.940024\n",
      "Training average negative_sample_loss at step 90: 0.531090\n",
      "Training average loss at step 90: 1.471114\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 100: 0.903453\n",
      "Training average negative_sample_loss at step 100: 0.549213\n",
      "Training average loss at step 100: 1.452665\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 100: 0.165444\n",
      "Valid MR at step 100: 7.670854\n",
      "Valid HITS@1 at step 100: 0.000000\n",
      "Valid HITS@3 at step 100: 0.100503\n",
      "Valid HITS@10 at step 100: 0.798995\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 110: 0.871853\n",
      "Training average negative_sample_loss at step 110: 0.564582\n",
      "Training average loss at step 110: 1.436435\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 120: 0.844011\n",
      "Training average negative_sample_loss at step 120: 0.577904\n",
      "Training average loss at step 120: 1.421915\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 120: 0.168975\n",
      "Valid MR at step 120: 7.500000\n",
      "Valid HITS@1 at step 120: 0.000000\n",
      "Valid HITS@3 at step 120: 0.105528\n",
      "Valid HITS@10 at step 120: 0.809045\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 130: 0.820670\n",
      "Training average negative_sample_loss at step 130: 0.588942\n",
      "Training average loss at step 130: 1.409612\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 140: 0.800092\n",
      "Training average negative_sample_loss at step 140: 0.598909\n",
      "Training average loss at step 140: 1.399001\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 140: 0.171913\n",
      "Valid MR at step 140: 7.366834\n",
      "Valid HITS@1 at step 140: 0.000000\n",
      "Valid HITS@3 at step 140: 0.110553\n",
      "Valid HITS@10 at step 140: 0.816583\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 150: 0.783522\n",
      "Training average negative_sample_loss at step 150: 0.606438\n",
      "Training average loss at step 150: 1.389961\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 160: 0.768254\n",
      "Training average negative_sample_loss at step 160: 0.612890\n",
      "Training average loss at step 160: 1.381144\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 160: 0.175227\n",
      "Valid MR at step 160: 7.266332\n",
      "Valid HITS@1 at step 160: 0.000000\n",
      "Valid HITS@3 at step 160: 0.123116\n",
      "Valid HITS@10 at step 160: 0.824121\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 170: 0.756286\n",
      "Training average negative_sample_loss at step 170: 0.617703\n",
      "Training average loss at step 170: 1.373988\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 180: 0.745481\n",
      "Training average negative_sample_loss at step 180: 0.621562\n",
      "Training average loss at step 180: 1.367043\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 180: 0.179122\n",
      "Valid MR at step 180: 7.150754\n",
      "Valid HITS@1 at step 180: 0.000000\n",
      "Valid HITS@3 at step 180: 0.133166\n",
      "Valid HITS@10 at step 180: 0.834171\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 190: 0.736672\n",
      "Training average negative_sample_loss at step 190: 0.624256\n",
      "Training average loss at step 190: 1.360929\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 200: 0.729390\n",
      "Training average negative_sample_loss at step 200: 0.626457\n",
      "Training average loss at step 200: 1.355846\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 200: 0.180929\n",
      "Valid MR at step 200: 7.077889\n",
      "Valid HITS@1 at step 200: 0.000000\n",
      "Valid HITS@3 at step 200: 0.133166\n",
      "Valid HITS@10 at step 200: 0.846734\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 210: 0.721810\n",
      "Training average negative_sample_loss at step 210: 0.627933\n",
      "Training average loss at step 210: 1.349743\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 220: 0.715966\n",
      "Training average negative_sample_loss at step 220: 0.629079\n",
      "Training average loss at step 220: 1.345045\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 220: 0.179871\n",
      "Valid MR at step 220: 7.097990\n",
      "Valid HITS@1 at step 220: 0.000000\n",
      "Valid HITS@3 at step 220: 0.125628\n",
      "Valid HITS@10 at step 220: 0.841709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 230: 0.710660\n",
      "Training average negative_sample_loss at step 230: 0.629831\n",
      "Training average loss at step 230: 1.340492\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 240: 0.707028\n",
      "Training average negative_sample_loss at step 240: 0.629188\n",
      "Training average loss at step 240: 1.336217\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 240: 0.183043\n",
      "Valid MR at step 240: 7.005025\n",
      "Valid HITS@1 at step 240: 0.000000\n",
      "Valid HITS@3 at step 240: 0.130653\n",
      "Valid HITS@10 at step 240: 0.841709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 250: 0.702991\n",
      "Training average negative_sample_loss at step 250: 0.629302\n",
      "Training average loss at step 250: 1.332293\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 260: 0.699225\n",
      "Training average negative_sample_loss at step 260: 0.629374\n",
      "Training average loss at step 260: 1.328599\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 260: 0.183661\n",
      "Valid MR at step 260: 6.964824\n",
      "Valid HITS@1 at step 260: 0.000000\n",
      "Valid HITS@3 at step 260: 0.140704\n",
      "Valid HITS@10 at step 260: 0.851759\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 270: 0.696118\n",
      "Training average negative_sample_loss at step 270: 0.628830\n",
      "Training average loss at step 270: 1.324948\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 280: 0.693767\n",
      "Training average negative_sample_loss at step 280: 0.628425\n",
      "Training average loss at step 280: 1.322192\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 280: 0.185445\n",
      "Valid MR at step 280: 6.942211\n",
      "Valid HITS@1 at step 280: 0.000000\n",
      "Valid HITS@3 at step 280: 0.143216\n",
      "Valid HITS@10 at step 280: 0.851759\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 290: 0.690362\n",
      "Training average negative_sample_loss at step 290: 0.628143\n",
      "Training average loss at step 290: 1.318505\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 300: 0.687955\n",
      "Training average negative_sample_loss at step 300: 0.627509\n",
      "Training average loss at step 300: 1.315464\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 300: 0.186077\n",
      "Valid MR at step 300: 6.902010\n",
      "Valid HITS@1 at step 300: 0.000000\n",
      "Valid HITS@3 at step 300: 0.145729\n",
      "Valid HITS@10 at step 300: 0.854271\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 310: 0.686368\n",
      "Training average negative_sample_loss at step 310: 0.626680\n",
      "Training average loss at step 310: 1.313048\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 320: 0.685340\n",
      "Training average negative_sample_loss at step 320: 0.625729\n",
      "Training average loss at step 320: 1.311069\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 320: 0.186882\n",
      "Valid MR at step 320: 6.891960\n",
      "Valid HITS@1 at step 320: 0.000000\n",
      "Valid HITS@3 at step 320: 0.145729\n",
      "Valid HITS@10 at step 320: 0.854271\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 330: 0.682997\n",
      "Training average negative_sample_loss at step 330: 0.624834\n",
      "Training average loss at step 330: 1.307831\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 340: 0.681151\n",
      "Training average negative_sample_loss at step 340: 0.624617\n",
      "Training average loss at step 340: 1.305767\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 340: 0.188246\n",
      "Valid MR at step 340: 6.871859\n",
      "Valid HITS@1 at step 340: 0.000000\n",
      "Valid HITS@3 at step 340: 0.150754\n",
      "Valid HITS@10 at step 340: 0.856784\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 350: 0.681008\n",
      "Training average negative_sample_loss at step 350: 0.623548\n",
      "Training average loss at step 350: 1.304556\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 360: 0.678815\n",
      "Training average negative_sample_loss at step 360: 0.622424\n",
      "Training average loss at step 360: 1.301240\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 360: 0.192207\n",
      "Valid MR at step 360: 6.814070\n",
      "Valid HITS@1 at step 360: 0.002513\n",
      "Valid HITS@3 at step 360: 0.158291\n",
      "Valid HITS@10 at step 360: 0.854271\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 370: 0.677298\n",
      "Training average negative_sample_loss at step 370: 0.621956\n",
      "Training average loss at step 370: 1.299255\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 380: 0.677110\n",
      "Training average negative_sample_loss at step 380: 0.620948\n",
      "Training average loss at step 380: 1.298058\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 380: 0.193121\n",
      "Valid MR at step 380: 6.809045\n",
      "Valid HITS@1 at step 380: 0.002513\n",
      "Valid HITS@3 at step 380: 0.160804\n",
      "Valid HITS@10 at step 380: 0.851759\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 390: 0.675069\n",
      "Training average negative_sample_loss at step 390: 0.620055\n",
      "Training average loss at step 390: 1.295123\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 400: 0.674830\n",
      "Training average negative_sample_loss at step 400: 0.619051\n",
      "Training average loss at step 400: 1.293881\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 400: 0.193508\n",
      "Valid MR at step 400: 6.821608\n",
      "Valid HITS@1 at step 400: 0.002513\n",
      "Valid HITS@3 at step 400: 0.160804\n",
      "Valid HITS@10 at step 400: 0.849246\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 410: 0.673942\n",
      "Training average negative_sample_loss at step 410: 0.618433\n",
      "Training average loss at step 410: 1.292375\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 420: 0.673471\n",
      "Training average negative_sample_loss at step 420: 0.617063\n",
      "Training average loss at step 420: 1.290533\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 420: 0.193610\n",
      "Valid MR at step 420: 6.811558\n",
      "Valid HITS@1 at step 420: 0.002513\n",
      "Valid HITS@3 at step 420: 0.160804\n",
      "Valid HITS@10 at step 420: 0.854271\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 430: 0.673055\n",
      "Training average negative_sample_loss at step 430: 0.616332\n",
      "Training average loss at step 430: 1.289387\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 440: 0.672026\n",
      "Training average negative_sample_loss at step 440: 0.615932\n",
      "Training average loss at step 440: 1.287958\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 440: 0.194191\n",
      "Valid MR at step 440: 6.766332\n",
      "Valid HITS@1 at step 440: 0.002513\n",
      "Valid HITS@3 at step 440: 0.158291\n",
      "Valid HITS@10 at step 440: 0.861809\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 450: 0.670810\n",
      "Training average negative_sample_loss at step 450: 0.615350\n",
      "Training average loss at step 450: 1.286160\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 460: 0.670159\n",
      "Training average negative_sample_loss at step 460: 0.614837\n",
      "Training average loss at step 460: 1.284996\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 460: 0.194562\n",
      "Valid MR at step 460: 6.736181\n",
      "Valid HITS@1 at step 460: 0.002513\n",
      "Valid HITS@3 at step 460: 0.158291\n",
      "Valid HITS@10 at step 460: 0.861809\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 470: 0.669746\n",
      "Training average negative_sample_loss at step 470: 0.613713\n",
      "Training average loss at step 470: 1.283459\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 480: 0.668560\n",
      "Training average negative_sample_loss at step 480: 0.613068\n",
      "Training average loss at step 480: 1.281628\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 480: 0.194572\n",
      "Valid MR at step 480: 6.723618\n",
      "Valid HITS@1 at step 480: 0.002513\n",
      "Valid HITS@3 at step 480: 0.160804\n",
      "Valid HITS@10 at step 480: 0.866834\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 490: 0.668369\n",
      "Training average negative_sample_loss at step 490: 0.612143\n",
      "Training average loss at step 490: 1.280512\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 500: 0.668159\n",
      "Training average negative_sample_loss at step 500: 0.611384\n",
      "Training average loss at step 500: 1.279543\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 500: 0.194605\n",
      "Valid MR at step 500: 6.728643\n",
      "Valid HITS@1 at step 500: 0.002513\n",
      "Valid HITS@3 at step 500: 0.163317\n",
      "Valid HITS@10 at step 500: 0.864322\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 510: 0.667806\n",
      "Training average negative_sample_loss at step 510: 0.610649\n",
      "Training average loss at step 510: 1.278455\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 520: 0.667330\n",
      "Training average negative_sample_loss at step 520: 0.609432\n",
      "Training average loss at step 520: 1.276762\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 520: 0.194699\n",
      "Valid MR at step 520: 6.733668\n",
      "Valid HITS@1 at step 520: 0.002513\n",
      "Valid HITS@3 at step 520: 0.165829\n",
      "Valid HITS@10 at step 520: 0.864322\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 530: 0.667833\n",
      "Training average negative_sample_loss at step 530: 0.609018\n",
      "Training average loss at step 530: 1.276851\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 540: 0.667460\n",
      "Training average negative_sample_loss at step 540: 0.608270\n",
      "Training average loss at step 540: 1.275730\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 540: 0.195305\n",
      "Valid MR at step 540: 6.736181\n",
      "Valid HITS@1 at step 540: 0.002513\n",
      "Valid HITS@3 at step 540: 0.170854\n",
      "Valid HITS@10 at step 540: 0.864322\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 550: 0.665473\n",
      "Training average negative_sample_loss at step 550: 0.608338\n",
      "Training average loss at step 550: 1.273811\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 560: 0.665899\n",
      "Training average negative_sample_loss at step 560: 0.607674\n",
      "Training average loss at step 560: 1.273573\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 560: 0.196065\n",
      "Valid MR at step 560: 6.716080\n",
      "Valid HITS@1 at step 560: 0.002513\n",
      "Valid HITS@3 at step 560: 0.173367\n",
      "Valid HITS@10 at step 560: 0.866834\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 570: 0.666027\n",
      "Training average negative_sample_loss at step 570: 0.606654\n",
      "Training average loss at step 570: 1.272680\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 580: 0.665243\n",
      "Training average negative_sample_loss at step 580: 0.606068\n",
      "Training average loss at step 580: 1.271310\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 580: 0.196398\n",
      "Valid MR at step 580: 6.693467\n",
      "Valid HITS@1 at step 580: 0.002513\n",
      "Valid HITS@3 at step 580: 0.173367\n",
      "Valid HITS@10 at step 580: 0.869347\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 590: 0.665686\n",
      "Training average negative_sample_loss at step 590: 0.605714\n",
      "Training average loss at step 590: 1.271400\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 600: 0.665812\n",
      "Training average negative_sample_loss at step 600: 0.604818\n",
      "Training average loss at step 600: 1.270630\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 600: 0.196059\n",
      "Valid MR at step 600: 6.690955\n",
      "Valid HITS@1 at step 600: 0.002513\n",
      "Valid HITS@3 at step 600: 0.173367\n",
      "Valid HITS@10 at step 600: 0.869347\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 610: 0.664698\n",
      "Training average negative_sample_loss at step 610: 0.604674\n",
      "Training average loss at step 610: 1.269372\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 620: 0.665029\n",
      "Training average negative_sample_loss at step 620: 0.603709\n",
      "Training average loss at step 620: 1.268738\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 620: 0.195953\n",
      "Valid MR at step 620: 6.690955\n",
      "Valid HITS@1 at step 620: 0.002513\n",
      "Valid HITS@3 at step 620: 0.173367\n",
      "Valid HITS@10 at step 620: 0.869347\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 630: 0.663846\n",
      "Training average negative_sample_loss at step 630: 0.603205\n",
      "Training average loss at step 630: 1.267052\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 640: 0.663423\n",
      "Training average negative_sample_loss at step 640: 0.602953\n",
      "Training average loss at step 640: 1.266376\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 640: 0.196922\n",
      "Valid MR at step 640: 6.675879\n",
      "Valid HITS@1 at step 640: 0.002513\n",
      "Valid HITS@3 at step 640: 0.175879\n",
      "Valid HITS@10 at step 640: 0.869347\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 650: 0.664516\n",
      "Training average negative_sample_loss at step 650: 0.602016\n",
      "Training average loss at step 650: 1.266532\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 660: 0.662987\n",
      "Training average negative_sample_loss at step 660: 0.601478\n",
      "Training average loss at step 660: 1.264465\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 660: 0.198753\n",
      "Valid MR at step 660: 6.623116\n",
      "Valid HITS@1 at step 660: 0.002513\n",
      "Valid HITS@3 at step 660: 0.178392\n",
      "Valid HITS@10 at step 660: 0.871859\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 670: 0.663879\n",
      "Training average negative_sample_loss at step 670: 0.601153\n",
      "Training average loss at step 670: 1.265032\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 680: 0.662880\n",
      "Training average negative_sample_loss at step 680: 0.601062\n",
      "Training average loss at step 680: 1.263942\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 680: 0.198684\n",
      "Valid MR at step 680: 6.620603\n",
      "Valid HITS@1 at step 680: 0.002513\n",
      "Valid HITS@3 at step 680: 0.180905\n",
      "Valid HITS@10 at step 680: 0.871859\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 690: 0.663490\n",
      "Training average negative_sample_loss at step 690: 0.600164\n",
      "Training average loss at step 690: 1.263654\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 700: 0.663023\n",
      "Training average negative_sample_loss at step 700: 0.599929\n",
      "Training average loss at step 700: 1.262951\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 700: 0.198945\n",
      "Valid MR at step 700: 6.618090\n",
      "Valid HITS@1 at step 700: 0.002513\n",
      "Valid HITS@3 at step 700: 0.183417\n",
      "Valid HITS@10 at step 700: 0.871859\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 710: 0.663244\n",
      "Training average negative_sample_loss at step 710: 0.599861\n",
      "Training average loss at step 710: 1.263105\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 720: 0.662831\n",
      "Training average negative_sample_loss at step 720: 0.599362\n",
      "Training average loss at step 720: 1.262193\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 720: 0.201967\n",
      "Valid MR at step 720: 6.577889\n",
      "Valid HITS@1 at step 720: 0.005025\n",
      "Valid HITS@3 at step 720: 0.190955\n",
      "Valid HITS@10 at step 720: 0.874372\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 730: 0.661823\n",
      "Training average negative_sample_loss at step 730: 0.599462\n",
      "Training average loss at step 730: 1.261285\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 740: 0.661304\n",
      "Training average negative_sample_loss at step 740: 0.598659\n",
      "Training average loss at step 740: 1.259963\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 740: 0.201613\n",
      "Valid MR at step 740: 6.572864\n",
      "Valid HITS@1 at step 740: 0.005025\n",
      "Valid HITS@3 at step 740: 0.188442\n",
      "Valid HITS@10 at step 740: 0.881910\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 750: 0.662606\n",
      "Training average negative_sample_loss at step 750: 0.598830\n",
      "Training average loss at step 750: 1.261436\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 760: 0.661150\n",
      "Training average negative_sample_loss at step 760: 0.598149\n",
      "Training average loss at step 760: 1.259300\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 760: 0.202551\n",
      "Valid MR at step 760: 6.565327\n",
      "Valid HITS@1 at step 760: 0.005025\n",
      "Valid HITS@3 at step 760: 0.193467\n",
      "Valid HITS@10 at step 760: 0.884422\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 770: 0.661268\n",
      "Training average negative_sample_loss at step 770: 0.597376\n",
      "Training average loss at step 770: 1.258644\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 780: 0.660777\n",
      "Training average negative_sample_loss at step 780: 0.596897\n",
      "Training average loss at step 780: 1.257674\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 780: 0.202382\n",
      "Valid MR at step 780: 6.560302\n",
      "Valid HITS@1 at step 780: 0.005025\n",
      "Valid HITS@3 at step 780: 0.193467\n",
      "Valid HITS@10 at step 780: 0.884422\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 790: 0.661609\n",
      "Training average negative_sample_loss at step 790: 0.596806\n",
      "Training average loss at step 790: 1.258415\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 800: 0.661448\n",
      "Training average negative_sample_loss at step 800: 0.595681\n",
      "Training average loss at step 800: 1.257128\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 800: 0.203991\n",
      "Valid MR at step 800: 6.567839\n",
      "Valid HITS@1 at step 800: 0.007538\n",
      "Valid HITS@3 at step 800: 0.193467\n",
      "Valid HITS@10 at step 800: 0.884422\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 810: 0.661456\n",
      "Training average negative_sample_loss at step 810: 0.595939\n",
      "Training average loss at step 810: 1.257395\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 820: 0.660633\n",
      "Training average negative_sample_loss at step 820: 0.596052\n",
      "Training average loss at step 820: 1.256685\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 820: 0.204252\n",
      "Valid MR at step 820: 6.555276\n",
      "Valid HITS@1 at step 820: 0.007538\n",
      "Valid HITS@3 at step 820: 0.193467\n",
      "Valid HITS@10 at step 820: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 830: 0.661377\n",
      "Training average negative_sample_loss at step 830: 0.595155\n",
      "Training average loss at step 830: 1.256532\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 840: 0.661707\n",
      "Training average negative_sample_loss at step 840: 0.594886\n",
      "Training average loss at step 840: 1.256592\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 840: 0.204377\n",
      "Valid MR at step 840: 6.552764\n",
      "Valid HITS@1 at step 840: 0.007538\n",
      "Valid HITS@3 at step 840: 0.193467\n",
      "Valid HITS@10 at step 840: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 850: 0.660569\n",
      "Training average negative_sample_loss at step 850: 0.595039\n",
      "Training average loss at step 850: 1.255608\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 860: 0.660737\n",
      "Training average negative_sample_loss at step 860: 0.594557\n",
      "Training average loss at step 860: 1.255293\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 860: 0.205013\n",
      "Valid MR at step 860: 6.542714\n",
      "Valid HITS@1 at step 860: 0.007538\n",
      "Valid HITS@3 at step 860: 0.193467\n",
      "Valid HITS@10 at step 860: 0.889447\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 870: 0.660558\n",
      "Training average negative_sample_loss at step 870: 0.593797\n",
      "Training average loss at step 870: 1.254355\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 880: 0.659714\n",
      "Training average negative_sample_loss at step 880: 0.593969\n",
      "Training average loss at step 880: 1.253683\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 880: 0.204391\n",
      "Valid MR at step 880: 6.557789\n",
      "Valid HITS@1 at step 880: 0.007538\n",
      "Valid HITS@3 at step 880: 0.190955\n",
      "Valid HITS@10 at step 880: 0.889447\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 890: 0.661050\n",
      "Training average negative_sample_loss at step 890: 0.593369\n",
      "Training average loss at step 890: 1.254420\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 900: 0.660720\n",
      "Training average negative_sample_loss at step 900: 0.593122\n",
      "Training average loss at step 900: 1.253842\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 900: 0.206681\n",
      "Valid MR at step 900: 6.532663\n",
      "Valid HITS@1 at step 900: 0.010050\n",
      "Valid HITS@3 at step 900: 0.190955\n",
      "Valid HITS@10 at step 900: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 910: 0.660295\n",
      "Training average negative_sample_loss at step 910: 0.592789\n",
      "Training average loss at step 910: 1.253084\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 920: 0.659702\n",
      "Training average negative_sample_loss at step 920: 0.592447\n",
      "Training average loss at step 920: 1.252149\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 920: 0.206472\n",
      "Valid MR at step 920: 6.537688\n",
      "Valid HITS@1 at step 920: 0.010050\n",
      "Valid HITS@3 at step 920: 0.190955\n",
      "Valid HITS@10 at step 920: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 930: 0.659772\n",
      "Training average negative_sample_loss at step 930: 0.592367\n",
      "Training average loss at step 930: 1.252138\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 940: 0.662219\n",
      "Training average negative_sample_loss at step 940: 0.592056\n",
      "Training average loss at step 940: 1.254275\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 940: 0.206081\n",
      "Valid MR at step 940: 6.535176\n",
      "Valid HITS@1 at step 940: 0.010050\n",
      "Valid HITS@3 at step 940: 0.188442\n",
      "Valid HITS@10 at step 940: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 950: 0.658962\n",
      "Training average negative_sample_loss at step 950: 0.592458\n",
      "Training average loss at step 950: 1.251419\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 960: 0.659210\n",
      "Training average negative_sample_loss at step 960: 0.592225\n",
      "Training average loss at step 960: 1.251435\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 960: 0.206760\n",
      "Valid MR at step 960: 6.520101\n",
      "Valid HITS@1 at step 960: 0.010050\n",
      "Valid HITS@3 at step 960: 0.190955\n",
      "Valid HITS@10 at step 960: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 970: 0.660261\n",
      "Training average negative_sample_loss at step 970: 0.591879\n",
      "Training average loss at step 970: 1.252140\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 980: 0.659556\n",
      "Training average negative_sample_loss at step 980: 0.591559\n",
      "Training average loss at step 980: 1.251115\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 980: 0.206710\n",
      "Valid MR at step 980: 6.517588\n",
      "Valid HITS@1 at step 980: 0.010050\n",
      "Valid HITS@3 at step 980: 0.190955\n",
      "Valid HITS@10 at step 980: 0.886935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 990: 0.660314\n",
      "Training average negative_sample_loss at step 990: 0.591316\n",
      "Training average loss at step 990: 1.251630\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 999: 0.207000\n",
      "Valid MR at step 999: 6.515075\n",
      "Valid HITS@1 at step 999: 0.010050\n",
      "Valid HITS@3 at step 999: 0.193467\n",
      "Valid HITS@10 at step 999: 0.886935\n",
      "Evaluating on Test Dataset...\n",
      "Evaluating the model... (0/402)\n",
      "Test MRR at step 999: 0.199919\n",
      "Test MR at step 999: 6.771144\n",
      "Test HITS@1 at step 999: 0.004975\n",
      "Test HITS@3 at step 999: 0.191542\n",
      "Test HITS@10 at step 999: 0.848259\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(parse_args(['--do_train', '--do_valid', '--do_test',\n",
    "                     '--model', 'TransE',\n",
    "                     '--max_steps', '1000', '--valid_steps', '20', '--log_steps', '10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: RotatE\n",
      "#entity: 14\n",
      "#relation: 55\n",
      "#train: 1592\n",
      "#valid: 199\n",
      "#test: 201\n",
      "\n",
      "Model Parameter Configuration:\n",
      "Parameter gamma: torch.Size([1]), require_grad = False\n",
      "Parameter embedding_range: torch.Size([1]), require_grad = False\n",
      "Parameter entity_embedding: torch.Size([14, 200]), require_grad = True\n",
      "Parameter relation_embedding: torch.Size([55, 100]), require_grad = True\n",
      "\n",
      "Evaluating initial model on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 1: 0.149310\n",
      "Valid MR at step 1: 8.268844\n",
      "Valid HITS@1 at step 1: 0.000000\n",
      "Valid HITS@3 at step 1: 0.067839\n",
      "Valid HITS@10 at step 1: 0.743719\n",
      "\n",
      "Start Training...\n",
      "init_step = 1\n",
      "batch_size = 1024\n",
      "hidden_dim = 100\n",
      "gamma = 2.000000\n",
      "learning_rate = 0.000100\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 10: 0.196188\n",
      "Training average negative_sample_loss at step 10: 1.781521\n",
      "Training average loss at step 10: 1.977709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 20: 0.199367\n",
      "Training average negative_sample_loss at step 20: 1.768591\n",
      "Training average loss at step 20: 1.967958\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 20: 0.149045\n",
      "Valid MR at step 20: 8.261307\n",
      "Valid HITS@1 at step 20: 0.000000\n",
      "Valid HITS@3 at step 20: 0.067839\n",
      "Valid HITS@10 at step 20: 0.753769\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 30: 0.202665\n",
      "Training average negative_sample_loss at step 30: 1.755981\n",
      "Training average loss at step 30: 1.958646\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 40: 0.206021\n",
      "Training average negative_sample_loss at step 40: 1.743348\n",
      "Training average loss at step 40: 1.949369\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 40: 0.149335\n",
      "Valid MR at step 40: 8.263819\n",
      "Valid HITS@1 at step 40: 0.000000\n",
      "Valid HITS@3 at step 40: 0.072864\n",
      "Valid HITS@10 at step 40: 0.763819\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 50: 0.209458\n",
      "Training average negative_sample_loss at step 50: 1.730143\n",
      "Training average loss at step 50: 1.939601\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 60: 0.212969\n",
      "Training average negative_sample_loss at step 60: 1.717101\n",
      "Training average loss at step 60: 1.930069\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 60: 0.150517\n",
      "Valid MR at step 60: 8.211055\n",
      "Valid HITS@1 at step 60: 0.000000\n",
      "Valid HITS@3 at step 60: 0.077889\n",
      "Valid HITS@10 at step 60: 0.771357\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 70: 0.216531\n",
      "Training average negative_sample_loss at step 70: 1.704110\n",
      "Training average loss at step 70: 1.920642\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 80: 0.220264\n",
      "Training average negative_sample_loss at step 80: 1.690901\n",
      "Training average loss at step 80: 1.911165\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 80: 0.149890\n",
      "Valid MR at step 80: 8.218593\n",
      "Valid HITS@1 at step 80: 0.000000\n",
      "Valid HITS@3 at step 80: 0.075377\n",
      "Valid HITS@10 at step 80: 0.768844\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 90: 0.223974\n",
      "Training average negative_sample_loss at step 90: 1.677164\n",
      "Training average loss at step 90: 1.901138\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 100: 0.227783\n",
      "Training average negative_sample_loss at step 100: 1.664432\n",
      "Training average loss at step 100: 1.892215\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 100: 0.149498\n",
      "Valid MR at step 100: 8.241206\n",
      "Valid HITS@1 at step 100: 0.000000\n",
      "Valid HITS@3 at step 100: 0.072864\n",
      "Valid HITS@10 at step 100: 0.761307\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 110: 0.231670\n",
      "Training average negative_sample_loss at step 110: 1.650735\n",
      "Training average loss at step 110: 1.882405\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 120: 0.235602\n",
      "Training average negative_sample_loss at step 120: 1.637680\n",
      "Training average loss at step 120: 1.873282\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 120: 0.149244\n",
      "Valid MR at step 120: 8.258794\n",
      "Valid HITS@1 at step 120: 0.000000\n",
      "Valid HITS@3 at step 120: 0.072864\n",
      "Valid HITS@10 at step 120: 0.756281\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 130: 0.239569\n",
      "Training average negative_sample_loss at step 130: 1.624987\n",
      "Training average loss at step 130: 1.864556\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 140: 0.243660\n",
      "Training average negative_sample_loss at step 140: 1.611768\n",
      "Training average loss at step 140: 1.855428\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 140: 0.149766\n",
      "Valid MR at step 140: 8.241206\n",
      "Valid HITS@1 at step 140: 0.000000\n",
      "Valid HITS@3 at step 140: 0.072864\n",
      "Valid HITS@10 at step 140: 0.751256\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 150: 0.247752\n",
      "Training average negative_sample_loss at step 150: 1.598242\n",
      "Training average loss at step 150: 1.845994\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 160: 0.251965\n",
      "Training average negative_sample_loss at step 160: 1.585412\n",
      "Training average loss at step 160: 1.837377\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 160: 0.148788\n",
      "Valid MR at step 160: 8.253769\n",
      "Valid HITS@1 at step 160: 0.000000\n",
      "Valid HITS@3 at step 160: 0.067839\n",
      "Valid HITS@10 at step 160: 0.758794\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 170: 0.256151\n",
      "Training average negative_sample_loss at step 170: 1.572759\n",
      "Training average loss at step 170: 1.828910\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 180: 0.260374\n",
      "Training average negative_sample_loss at step 180: 1.559796\n",
      "Training average loss at step 180: 1.820169\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 180: 0.149460\n",
      "Valid MR at step 180: 8.223618\n",
      "Valid HITS@1 at step 180: 0.000000\n",
      "Valid HITS@3 at step 180: 0.070352\n",
      "Valid HITS@10 at step 180: 0.763819\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 190: 0.264691\n",
      "Training average negative_sample_loss at step 190: 1.547656\n",
      "Training average loss at step 190: 1.812346\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 200: 0.268976\n",
      "Training average negative_sample_loss at step 200: 1.535087\n",
      "Training average loss at step 200: 1.804063\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 200: 0.149335\n",
      "Valid MR at step 200: 8.226131\n",
      "Valid HITS@1 at step 200: 0.000000\n",
      "Valid HITS@3 at step 200: 0.067839\n",
      "Valid HITS@10 at step 200: 0.766332\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 210: 0.273293\n",
      "Training average negative_sample_loss at step 210: 1.522662\n",
      "Training average loss at step 210: 1.795954\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 220: 0.277725\n",
      "Training average negative_sample_loss at step 220: 1.510940\n",
      "Training average loss at step 220: 1.788664\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 220: 0.149778\n",
      "Valid MR at step 220: 8.185930\n",
      "Valid HITS@1 at step 220: 0.000000\n",
      "Valid HITS@3 at step 220: 0.070352\n",
      "Valid HITS@10 at step 220: 0.771357\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 230: 0.282025\n",
      "Training average negative_sample_loss at step 230: 1.498424\n",
      "Training average loss at step 230: 1.780449\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 240: 0.286488\n",
      "Training average negative_sample_loss at step 240: 1.485855\n",
      "Training average loss at step 240: 1.772343\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 240: 0.151347\n",
      "Valid MR at step 240: 8.165829\n",
      "Valid HITS@1 at step 240: 0.000000\n",
      "Valid HITS@3 at step 240: 0.072864\n",
      "Valid HITS@10 at step 240: 0.771357\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 250: 0.290872\n",
      "Training average negative_sample_loss at step 250: 1.473787\n",
      "Training average loss at step 250: 1.764659\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 260: 0.295361\n",
      "Training average negative_sample_loss at step 260: 1.462193\n",
      "Training average loss at step 260: 1.757554\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 260: 0.153929\n",
      "Valid MR at step 260: 8.062814\n",
      "Valid HITS@1 at step 260: 0.000000\n",
      "Valid HITS@3 at step 260: 0.080402\n",
      "Valid HITS@10 at step 260: 0.788945\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 270: 0.299851\n",
      "Training average negative_sample_loss at step 270: 1.451141\n",
      "Training average loss at step 270: 1.750992\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 280: 0.304300\n",
      "Training average negative_sample_loss at step 280: 1.439455\n",
      "Training average loss at step 280: 1.743755\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 280: 0.154958\n",
      "Valid MR at step 280: 8.012563\n",
      "Valid HITS@1 at step 280: 0.000000\n",
      "Valid HITS@3 at step 280: 0.080402\n",
      "Valid HITS@10 at step 280: 0.793970\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 290: 0.308838\n",
      "Training average negative_sample_loss at step 290: 1.428268\n",
      "Training average loss at step 290: 1.737106\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 300: 0.313279\n",
      "Training average negative_sample_loss at step 300: 1.417130\n",
      "Training average loss at step 300: 1.730408\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 300: 0.155360\n",
      "Valid MR at step 300: 8.020101\n",
      "Valid HITS@1 at step 300: 0.000000\n",
      "Valid HITS@3 at step 300: 0.090452\n",
      "Valid HITS@10 at step 300: 0.793970\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 310: 0.317801\n",
      "Training average negative_sample_loss at step 310: 1.406247\n",
      "Training average loss at step 310: 1.724048\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 320: 0.322257\n",
      "Training average negative_sample_loss at step 320: 1.395554\n",
      "Training average loss at step 320: 1.717811\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 320: 0.157284\n",
      "Valid MR at step 320: 7.952261\n",
      "Valid HITS@1 at step 320: 0.000000\n",
      "Valid HITS@3 at step 320: 0.095477\n",
      "Valid HITS@10 at step 320: 0.798995\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 330: 0.326763\n",
      "Training average negative_sample_loss at step 330: 1.384847\n",
      "Training average loss at step 330: 1.711610\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 340: 0.331231\n",
      "Training average negative_sample_loss at step 340: 1.374423\n",
      "Training average loss at step 340: 1.705654\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 340: 0.159193\n",
      "Valid MR at step 340: 7.869347\n",
      "Valid HITS@1 at step 340: 0.000000\n",
      "Valid HITS@3 at step 340: 0.097990\n",
      "Valid HITS@10 at step 340: 0.798995\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 350: 0.335677\n",
      "Training average negative_sample_loss at step 350: 1.363946\n",
      "Training average loss at step 350: 1.699624\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 360: 0.340097\n",
      "Training average negative_sample_loss at step 360: 1.353668\n",
      "Training average loss at step 360: 1.693765\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 360: 0.159662\n",
      "Valid MR at step 360: 7.844221\n",
      "Valid HITS@1 at step 360: 0.000000\n",
      "Valid HITS@3 at step 360: 0.097990\n",
      "Valid HITS@10 at step 360: 0.801508\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 370: 0.344625\n",
      "Training average negative_sample_loss at step 370: 1.343813\n",
      "Training average loss at step 370: 1.688438\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 380: 0.349080\n",
      "Training average negative_sample_loss at step 380: 1.333281\n",
      "Training average loss at step 380: 1.682361\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 380: 0.161005\n",
      "Valid MR at step 380: 7.809045\n",
      "Valid HITS@1 at step 380: 0.000000\n",
      "Valid HITS@3 at step 380: 0.100503\n",
      "Valid HITS@10 at step 380: 0.798995\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 390: 0.353358\n",
      "Training average negative_sample_loss at step 390: 1.324038\n",
      "Training average loss at step 390: 1.677396\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 400: 0.357726\n",
      "Training average negative_sample_loss at step 400: 1.314813\n",
      "Training average loss at step 400: 1.672539\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 400: 0.160815\n",
      "Valid MR at step 400: 7.806533\n",
      "Valid HITS@1 at step 400: 0.000000\n",
      "Valid HITS@3 at step 400: 0.100503\n",
      "Valid HITS@10 at step 400: 0.801508\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 410: 0.362223\n",
      "Training average negative_sample_loss at step 410: 1.305867\n",
      "Training average loss at step 410: 1.668090\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 420: 0.366446\n",
      "Training average negative_sample_loss at step 420: 1.296423\n",
      "Training average loss at step 420: 1.662869\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 420: 0.162297\n",
      "Valid MR at step 420: 7.771357\n",
      "Valid HITS@1 at step 420: 0.000000\n",
      "Valid HITS@3 at step 420: 0.105528\n",
      "Valid HITS@10 at step 420: 0.806533\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 430: 0.370752\n",
      "Training average negative_sample_loss at step 430: 1.287066\n",
      "Training average loss at step 430: 1.657817\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 440: 0.375064\n",
      "Training average negative_sample_loss at step 440: 1.277846\n",
      "Training average loss at step 440: 1.652910\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 440: 0.162692\n",
      "Valid MR at step 440: 7.756281\n",
      "Valid HITS@1 at step 440: 0.000000\n",
      "Valid HITS@3 at step 440: 0.105528\n",
      "Valid HITS@10 at step 440: 0.811558\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 450: 0.379282\n",
      "Training average negative_sample_loss at step 450: 1.269573\n",
      "Training average loss at step 450: 1.648855\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 460: 0.383518\n",
      "Training average negative_sample_loss at step 460: 1.259839\n",
      "Training average loss at step 460: 1.643357\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 460: 0.164664\n",
      "Valid MR at step 460: 7.688442\n",
      "Valid HITS@1 at step 460: 0.000000\n",
      "Valid HITS@3 at step 460: 0.105528\n",
      "Valid HITS@10 at step 460: 0.816583\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 470: 0.387767\n",
      "Training average negative_sample_loss at step 470: 1.252315\n",
      "Training average loss at step 470: 1.640082\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 480: 0.391948\n",
      "Training average negative_sample_loss at step 480: 1.243898\n",
      "Training average loss at step 480: 1.635845\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 480: 0.166576\n",
      "Valid MR at step 480: 7.628141\n",
      "Valid HITS@1 at step 480: 0.000000\n",
      "Valid HITS@3 at step 480: 0.108040\n",
      "Valid HITS@10 at step 480: 0.821608\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 490: 0.395932\n",
      "Training average negative_sample_loss at step 490: 1.235890\n",
      "Training average loss at step 490: 1.631822\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 500: 0.399960\n",
      "Training average negative_sample_loss at step 500: 1.228779\n",
      "Training average loss at step 500: 1.628739\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 500: 0.168003\n",
      "Valid MR at step 500: 7.597990\n",
      "Valid HITS@1 at step 500: 0.000000\n",
      "Valid HITS@3 at step 500: 0.118090\n",
      "Valid HITS@10 at step 500: 0.824121\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 510: 0.404204\n",
      "Training average negative_sample_loss at step 510: 1.220242\n",
      "Training average loss at step 510: 1.624445\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 520: 0.408064\n",
      "Training average negative_sample_loss at step 520: 1.212545\n",
      "Training average loss at step 520: 1.620609\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 520: 0.168701\n",
      "Valid MR at step 520: 7.575377\n",
      "Valid HITS@1 at step 520: 0.000000\n",
      "Valid HITS@3 at step 520: 0.123116\n",
      "Valid HITS@10 at step 520: 0.824121\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 530: 0.411836\n",
      "Training average negative_sample_loss at step 530: 1.206359\n",
      "Training average loss at step 530: 1.618195\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 540: 0.415712\n",
      "Training average negative_sample_loss at step 540: 1.197755\n",
      "Training average loss at step 540: 1.613467\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 540: 0.169249\n",
      "Valid MR at step 540: 7.552764\n",
      "Valid HITS@1 at step 540: 0.000000\n",
      "Valid HITS@3 at step 540: 0.125628\n",
      "Valid HITS@10 at step 540: 0.824121\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 550: 0.419553\n",
      "Training average negative_sample_loss at step 550: 1.190565\n",
      "Training average loss at step 550: 1.610118\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 560: 0.423281\n",
      "Training average negative_sample_loss at step 560: 1.183347\n",
      "Training average loss at step 560: 1.606628\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 560: 0.168914\n",
      "Valid MR at step 560: 7.562814\n",
      "Valid HITS@1 at step 560: 0.000000\n",
      "Valid HITS@3 at step 560: 0.120603\n",
      "Valid HITS@10 at step 560: 0.819095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 570: 0.427162\n",
      "Training average negative_sample_loss at step 570: 1.176184\n",
      "Training average loss at step 570: 1.603346\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 580: 0.430765\n",
      "Training average negative_sample_loss at step 580: 1.169404\n",
      "Training average loss at step 580: 1.600169\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 580: 0.170210\n",
      "Valid MR at step 580: 7.530151\n",
      "Valid HITS@1 at step 580: 0.000000\n",
      "Valid HITS@3 at step 580: 0.123116\n",
      "Valid HITS@10 at step 580: 0.816583\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 590: 0.434591\n",
      "Training average negative_sample_loss at step 590: 1.163344\n",
      "Training average loss at step 590: 1.597935\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 600: 0.437875\n",
      "Training average negative_sample_loss at step 600: 1.157181\n",
      "Training average loss at step 600: 1.595056\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 600: 0.170221\n",
      "Valid MR at step 600: 7.515075\n",
      "Valid HITS@1 at step 600: 0.000000\n",
      "Valid HITS@3 at step 600: 0.118090\n",
      "Valid HITS@10 at step 600: 0.819095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 610: 0.441401\n",
      "Training average negative_sample_loss at step 610: 1.151148\n",
      "Training average loss at step 610: 1.592549\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 620: 0.445010\n",
      "Training average negative_sample_loss at step 620: 1.143944\n",
      "Training average loss at step 620: 1.588954\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 620: 0.172727\n",
      "Valid MR at step 620: 7.467337\n",
      "Valid HITS@1 at step 620: 0.000000\n",
      "Valid HITS@3 at step 620: 0.120603\n",
      "Valid HITS@10 at step 620: 0.819095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 630: 0.448244\n",
      "Training average negative_sample_loss at step 630: 1.137915\n",
      "Training average loss at step 630: 1.586159\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 640: 0.451757\n",
      "Training average negative_sample_loss at step 640: 1.131615\n",
      "Training average loss at step 640: 1.583372\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 640: 0.174800\n",
      "Valid MR at step 640: 7.417085\n",
      "Valid HITS@1 at step 640: 0.000000\n",
      "Valid HITS@3 at step 640: 0.130653\n",
      "Valid HITS@10 at step 640: 0.819095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 650: 0.454959\n",
      "Training average negative_sample_loss at step 650: 1.126720\n",
      "Training average loss at step 650: 1.581679\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 660: 0.458521\n",
      "Training average negative_sample_loss at step 660: 1.121056\n",
      "Training average loss at step 660: 1.579577\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 660: 0.176758\n",
      "Valid MR at step 660: 7.359296\n",
      "Valid HITS@1 at step 660: 0.000000\n",
      "Valid HITS@3 at step 660: 0.133166\n",
      "Valid HITS@10 at step 660: 0.821608\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 670: 0.461619\n",
      "Training average negative_sample_loss at step 670: 1.113805\n",
      "Training average loss at step 670: 1.575424\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 680: 0.464613\n",
      "Training average negative_sample_loss at step 680: 1.109255\n",
      "Training average loss at step 680: 1.573868\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 680: 0.176227\n",
      "Valid MR at step 680: 7.371859\n",
      "Valid HITS@1 at step 680: 0.000000\n",
      "Valid HITS@3 at step 680: 0.130653\n",
      "Valid HITS@10 at step 680: 0.819095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 690: 0.467812\n",
      "Training average negative_sample_loss at step 690: 1.104156\n",
      "Training average loss at step 690: 1.571968\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 700: 0.470660\n",
      "Training average negative_sample_loss at step 700: 1.099056\n",
      "Training average loss at step 700: 1.569715\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 700: 0.177327\n",
      "Valid MR at step 700: 7.356784\n",
      "Valid HITS@1 at step 700: 0.000000\n",
      "Valid HITS@3 at step 700: 0.135678\n",
      "Valid HITS@10 at step 700: 0.816583\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 710: 0.473821\n",
      "Training average negative_sample_loss at step 710: 1.094401\n",
      "Training average loss at step 710: 1.568222\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 720: 0.476579\n",
      "Training average negative_sample_loss at step 720: 1.087571\n",
      "Training average loss at step 720: 1.564150\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 720: 0.177428\n",
      "Valid MR at step 720: 7.359296\n",
      "Valid HITS@1 at step 720: 0.000000\n",
      "Valid HITS@3 at step 720: 0.135678\n",
      "Valid HITS@10 at step 720: 0.814070\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 730: 0.479471\n",
      "Training average negative_sample_loss at step 730: 1.083586\n",
      "Training average loss at step 730: 1.563056\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 740: 0.482342\n",
      "Training average negative_sample_loss at step 740: 1.078574\n",
      "Training average loss at step 740: 1.560916\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 740: 0.177672\n",
      "Valid MR at step 740: 7.361809\n",
      "Valid HITS@1 at step 740: 0.000000\n",
      "Valid HITS@3 at step 740: 0.140704\n",
      "Valid HITS@10 at step 740: 0.814070\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 750: 0.485170\n",
      "Training average negative_sample_loss at step 750: 1.073901\n",
      "Training average loss at step 750: 1.559072\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 760: 0.487801\n",
      "Training average negative_sample_loss at step 760: 1.069392\n",
      "Training average loss at step 760: 1.557193\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 760: 0.177604\n",
      "Valid MR at step 760: 7.356784\n",
      "Valid HITS@1 at step 760: 0.000000\n",
      "Valid HITS@3 at step 760: 0.140704\n",
      "Valid HITS@10 at step 760: 0.821608\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 770: 0.490715\n",
      "Training average negative_sample_loss at step 770: 1.064769\n",
      "Training average loss at step 770: 1.555484\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 780: 0.493082\n",
      "Training average negative_sample_loss at step 780: 1.059996\n",
      "Training average loss at step 780: 1.553077\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 780: 0.176919\n",
      "Valid MR at step 780: 7.359296\n",
      "Valid HITS@1 at step 780: 0.000000\n",
      "Valid HITS@3 at step 780: 0.138191\n",
      "Valid HITS@10 at step 780: 0.821608\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 790: 0.495643\n",
      "Training average negative_sample_loss at step 790: 1.055642\n",
      "Training average loss at step 790: 1.551285\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 800: 0.497805\n",
      "Training average negative_sample_loss at step 800: 1.051888\n",
      "Training average loss at step 800: 1.549692\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 800: 0.176786\n",
      "Valid MR at step 800: 7.359296\n",
      "Valid HITS@1 at step 800: 0.000000\n",
      "Valid HITS@3 at step 800: 0.138191\n",
      "Valid HITS@10 at step 800: 0.826633\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 810: 0.500467\n",
      "Training average negative_sample_loss at step 810: 1.048628\n",
      "Training average loss at step 810: 1.549095\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 820: 0.503086\n",
      "Training average negative_sample_loss at step 820: 1.044139\n",
      "Training average loss at step 820: 1.547225\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 820: 0.177259\n",
      "Valid MR at step 820: 7.344221\n",
      "Valid HITS@1 at step 820: 0.000000\n",
      "Valid HITS@3 at step 820: 0.140704\n",
      "Valid HITS@10 at step 820: 0.829146\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 830: 0.505263\n",
      "Training average negative_sample_loss at step 830: 1.040218\n",
      "Training average loss at step 830: 1.545481\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 840: 0.507652\n",
      "Training average negative_sample_loss at step 840: 1.036075\n",
      "Training average loss at step 840: 1.543727\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 840: 0.177882\n",
      "Valid MR at step 840: 7.341709\n",
      "Valid HITS@1 at step 840: 0.000000\n",
      "Valid HITS@3 at step 840: 0.148241\n",
      "Valid HITS@10 at step 840: 0.829146\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 850: 0.510119\n",
      "Training average negative_sample_loss at step 850: 1.032951\n",
      "Training average loss at step 850: 1.543070\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 860: 0.512056\n",
      "Training average negative_sample_loss at step 860: 1.028258\n",
      "Training average loss at step 860: 1.540314\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 860: 0.178305\n",
      "Valid MR at step 860: 7.324121\n",
      "Valid HITS@1 at step 860: 0.000000\n",
      "Valid HITS@3 at step 860: 0.150754\n",
      "Valid HITS@10 at step 860: 0.834171\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 870: 0.514218\n",
      "Training average negative_sample_loss at step 870: 1.024904\n",
      "Training average loss at step 870: 1.539122\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 880: 0.516495\n",
      "Training average negative_sample_loss at step 880: 1.022072\n",
      "Training average loss at step 880: 1.538567\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 880: 0.178515\n",
      "Valid MR at step 880: 7.301508\n",
      "Valid HITS@1 at step 880: 0.000000\n",
      "Valid HITS@3 at step 880: 0.150754\n",
      "Valid HITS@10 at step 880: 0.839196\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 890: 0.518412\n",
      "Training average negative_sample_loss at step 890: 1.017783\n",
      "Training average loss at step 890: 1.536195\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 900: 0.520739\n",
      "Training average negative_sample_loss at step 900: 1.014353\n",
      "Training average loss at step 900: 1.535093\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 900: 0.178485\n",
      "Valid MR at step 900: 7.293970\n",
      "Valid HITS@1 at step 900: 0.000000\n",
      "Valid HITS@3 at step 900: 0.150754\n",
      "Valid HITS@10 at step 900: 0.841709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 910: 0.522667\n",
      "Training average negative_sample_loss at step 910: 1.011068\n",
      "Training average loss at step 910: 1.533735\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 920: 0.524125\n",
      "Training average negative_sample_loss at step 920: 1.008314\n",
      "Training average loss at step 920: 1.532439\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 920: 0.178282\n",
      "Valid MR at step 920: 7.296482\n",
      "Valid HITS@1 at step 920: 0.000000\n",
      "Valid HITS@3 at step 920: 0.150754\n",
      "Valid HITS@10 at step 920: 0.841709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 930: 0.525908\n",
      "Training average negative_sample_loss at step 930: 1.005358\n",
      "Training average loss at step 930: 1.531266\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 940: 0.527944\n",
      "Training average negative_sample_loss at step 940: 1.001536\n",
      "Training average loss at step 940: 1.529480\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 940: 0.177334\n",
      "Valid MR at step 940: 7.298995\n",
      "Valid HITS@1 at step 940: 0.000000\n",
      "Valid HITS@3 at step 940: 0.145729\n",
      "Valid HITS@10 at step 940: 0.839196\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 950: 0.529978\n",
      "Training average negative_sample_loss at step 950: 0.997769\n",
      "Training average loss at step 950: 1.527747\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 960: 0.532206\n",
      "Training average negative_sample_loss at step 960: 0.995298\n",
      "Training average loss at step 960: 1.527504\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 960: 0.177479\n",
      "Valid MR at step 960: 7.286432\n",
      "Valid HITS@1 at step 960: 0.000000\n",
      "Valid HITS@3 at step 960: 0.145729\n",
      "Valid HITS@10 at step 960: 0.841709\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 970: 0.533380\n",
      "Training average negative_sample_loss at step 970: 0.992729\n",
      "Training average loss at step 970: 1.526108\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 980: 0.534959\n",
      "Training average negative_sample_loss at step 980: 0.989934\n",
      "Training average loss at step 980: 1.524892\n",
      "\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 980: 0.177387\n",
      "Valid MR at step 980: 7.291457\n",
      "Valid HITS@1 at step 980: 0.000000\n",
      "Valid HITS@3 at step 980: 0.145729\n",
      "Valid HITS@10 at step 980: 0.839196\n",
      "\n",
      "Training metrics...\n",
      "Training average positive_sample_loss at step 990: 0.536667\n",
      "Training average negative_sample_loss at step 990: 0.987605\n",
      "Training average loss at step 990: 1.524273\n",
      "Evaluating on Valid Dataset...\n",
      "Evaluating the model... (0/398)\n",
      "Valid MRR at step 999: 0.176901\n",
      "Valid MR at step 999: 7.298995\n",
      "Valid HITS@1 at step 999: 0.000000\n",
      "Valid HITS@3 at step 999: 0.145729\n",
      "Valid HITS@10 at step 999: 0.841709\n",
      "Evaluating on Test Dataset...\n",
      "Evaluating the model... (0/402)\n",
      "Test MRR at step 999: 0.179405\n",
      "Test MR at step 999: 7.410448\n",
      "Test HITS@1 at step 999: 0.000000\n",
      "Test HITS@3 at step 999: 0.151741\n",
      "Test HITS@10 at step 999: 0.796020\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main(parse_args(['--do_train', '--do_valid', '--do_test',\n",
    "                     '--model', 'RotatE',\n",
    "                     '--max_steps', '1000', '--valid_steps', '20', '--log_steps', '10',\n",
    "                     '--double_entity_embedding']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may notice that both the Valid and Test HITS@1 are 0 for the RotatE model. I do think this is just the limitation of this implementation rather than flaws in the coding."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
