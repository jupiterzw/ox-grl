{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qBOS0ukjxIQ"
   },
   "source": [
    "# Introduction\n",
    "Welcome to Practical 3 for Graph Representation Learning.  In this practical, you are expected to first implement two functions from scratch: **Graph Mini-Batching** and **Global Pooling**. Then, you need to incorporate these two functions into a graph neural network model to solve a **Graph Classification** task.\n",
    "\n",
    "- **Graph Mini-Batching** A mini-batch groups a set of graphs into a unified representation where it can efficiently be processed in parallel.\n",
    "- **Global Pooling** Obtain the graph feature based on all node features in the graph, in which you can use different operations such as summation, mean and max.\n",
    "\n",
    "We will be using [PyTorch](https://pytorch.org/docs/stable/index.html) and [PyG](https://pytorch-geometric.readthedocs.io/en/latest/) for experiments.\n",
    "\n",
    "The notebook is divided into sections, each of which comes with complete or partially completed code. Before each snippet of code there will be a description of what we are about to implement. The sections of code you need to complete are marked as **Tasks**.\n",
    "\n",
    "Please ensure that you operate within the framework given in the notebook and bring any questions you may have to the practical demonstrators. We suggest that you **DO NOT** edit code that is a part of the framework, since this will make it more difficult for demonstrators to assist if your code is broken.\n",
    "\n",
    "Since we are working in a Jupyter Notebook, the code is very interactive. When you're stuck on something, try adding a new block of code below what you're working on and using it to debug your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWwo4htCSNj4"
   },
   "source": [
    "## Installing dependencies\n",
    "First of all, we advise you to enable GPU acceleration for your notebook. This can be done by navigating to `Runtime > Change runtime type > Hardware accelerator (GPU) > Save`. You may getting an error explaining that no GPUs are currently available. This is fine, you don't really need them for this practical, however they'll make your computations significantly faster.\n",
    "\n",
    "Some other tips & tricks:\n",
    "- press `Shift + Enter` to run a cell and move to the next one (`Ctrl + Enter` to only run it)\n",
    "- when you execute a cell, the variables you create are saved into a global namespace. As a consequence, changes in the code will not take effect until you re-run that specific cell.\n",
    "- remember to save your notebook every once in a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tOALkyBjnFbj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/pyg-team/pytorch_geometric.git\n",
      "  Cloning https://github.com/pyg-team/pytorch_geometric.git to /private/var/folders/4x/1dlwhbps0mgf3z6747y6zf6h0000gn/T/pip-req-build-534v54gc\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pyg-team/pytorch_geometric.git /private/var/folders/4x/1dlwhbps0mgf3z6747y6zf6h0000gn/T/pip-req-build-534v54gc\n",
      "  Resolved https://github.com/pyg-team/pytorch_geometric.git to commit 46705844b39ededc0fcef1de90e73923480a6446\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (3.11.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (3.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (5.9.0)\n",
      "Requirement already satisfied: pyparsing in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (3.2.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from torch-geometric==2.7.0) (4.66.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (1.17.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from aiohttp->torch-geometric==2.7.0) (5.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from jinja2->torch-geometric==2.7.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from requests->torch-geometric==2.7.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from requests->torch-geometric==2.7.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from requests->torch-geometric==2.7.0) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from requests->torch-geometric==2.7.0) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/anaconda3/envs/GRL/lib/python3.9/site-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric==2.7.0) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "# Download the corresponding PyTorch Geometric module\n",
    "# %%capture\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkz6VLjOryE5"
   },
   "source": [
    "# Imports\n",
    "\n",
    "Run the following blocks of code to install and import and the necessary python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wY3B_uB3ry43"
   },
   "outputs": [],
   "source": [
    "# Let's first import all the things we are gonna need for this task\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import scatter\n",
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric.utils as U\n",
    "# torch_geometric only used to load the Cora dataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data.data import Data\n",
    "import torch_geometric.utils as U\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.datasets import TUDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZKndO0hYiKr"
   },
   "source": [
    "\n",
    "# Dataset\n",
    "\n",
    "## Loading MUTAG from TUDataset\n",
    "We will use TUDataset to load the MUTAG dataset, which contains molecular graphs of 188 chemical compounds divided into two classes according to their mutagenic effect on a bacterium.\n",
    "\n",
    "We construct three lists: A, X, Y, which correspond to a list of adj_matrix, a list of node features and a list of graph labels, respectively.\n",
    "\n",
    "Please **DO NOT** modify any part of the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xCTdKEvIobt6"
   },
   "outputs": [],
   "source": [
    "# please **DO NOT** modify  any part of the following code  in this cell\n",
    "raw_dataset = TUDataset(root='data/TUDataset', name='MUTAG') # raw_dataset is an instance of class TUDataset\n",
    "batch_size = 32\n",
    "A = []\n",
    "X = []\n",
    "Y = []\n",
    "# raw_dataset satisfies the Python iterable protocol (need to check details)\n",
    "for graph in raw_dataset: \n",
    "    adj_matrix = U.to_dense_adj(graph.edge_index).squeeze(0)\n",
    "    A.append(adj_matrix)\n",
    "    X.append(graph.x)\n",
    "    Y.append(graph.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on data structure**\n",
    "\n",
    "The next code cell outputs `Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])`, the form of a single graph in this dataset.\n",
    "- `edge_index=[2, 38]` is the edge list in COO (Coordinate) format where\n",
    "  - $2$ indicates the rows: one row for the source nodes and one for the destination nodes;\n",
    "  - $38$ is the total number of edges in the graph.\n",
    "- `x=[17, 7]` is the node feature matrix where\n",
    "  - $17$ is the number of nodes in this particular graph;\n",
    "  - $7$ is the feature dimension of all nodes.\n",
    "- `edge_attr=[38, 4]` is the edge feature matrix where\n",
    "  - $38$ is the total number of edges in the graph;\n",
    "  - $4$ is the feature dimension of all edges.\n",
    "- `y=[1]` is the binary label of this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 38], x=[17, 7], edge_attr=[38, 4], y=[1])\n"
     ]
    }
   ],
   "source": [
    "for graph in raw_dataset:\n",
    "    print(graph)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhI8VnJCvOf0"
   },
   "source": [
    "We can then run the following cell to output the first one element in the three constructed lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "VjKHlRk2tbxU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "print(A[0]) # a single adjacency matrix\n",
    "print(X[0]) # a single matrix of node features\n",
    "print(Y[0]) # a single number of graph label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17, 17])\n",
      "torch.Size([17, 7])\n",
      "torch.Size([13, 7])\n",
      "torch.Size([13, 7])\n",
      "torch.Size([16, 7])\n"
     ]
    }
   ],
   "source": [
    "# Note that the initial feature dimension is 7\n",
    "# the first number e.g. 17, 13, etc denote the number of nodes in this particular graph\n",
    "print(A[0].size())\n",
    "print(X[0].size())\n",
    "print(X[1].size())\n",
    "print(X[2].size())\n",
    "print(X[187].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_adj = len(A)\n",
    "num_adj # molecular graphs of 188 chemical compounds divided into two classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UlV9O3MORCTm"
   },
   "source": [
    "## Task 1\n",
    "Define an iterator `graph_mini_batch` that takes in a list of adj_matrix (A), a list of node features (X), a list of graph labels (Y) and a batch_size B=64, and outputs four items $A_B$, $X_B$, $Y_B$ and $\\textsf{Batch}$ each time, such that the batched Adjacency matrices  $A_B$ are stacked in a diagonal fashion (creating a giant graph that holds multiple isolated subgraphs), and the batched node features list $X_B$ and the batched graph label list $Y_B$ are simply concatenated in the node dimension, i.e.,\n",
    "\n",
    "\\begin{split}\\mathbf{A_B} = \\begin{bmatrix} \\mathbf{A}_1 & & \\\\ & \\ddots & \\\\ & & \\mathbf{A}_n \\end{bmatrix}, \\qquad \\mathbf{X_B} = \\begin{bmatrix} \\mathbf{X}_1 \\\\ \\vdots \\\\ \\mathbf{X}_n \\end{bmatrix}, \\qquad \\mathbf{Y_B} = \\begin{bmatrix} \\mathbf{Y}_1 \\\\ \\vdots \\\\ \\mathbf{Y}_n \\end{bmatrix}.\\end{split}\n",
    "\n",
    "Furthermore, you are expected to output a  **`Batch` vector**, which maps each node to its respective graph in the batch:\n",
    "\n",
    "$$\n",
    "\\textrm{Batch} = [ 0, \\ldots, 0, 1, \\ldots, 1, 2, \\ldots, n, \\ldots, n]\n",
    "$$\n",
    "\n",
    "**Hints:**\n",
    "1. use the keyword **yield** to make your function be an iterator.\n",
    "2. note that the last batch might not have the enough items satisfying the specified batch size, so your function should be able to deal with such a case and have a correct output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*More on the `Batch` vector*\n",
    "\n",
    "Suppose we have three graphs, each with the following nodes:\n",
    "- Graph 0: 3 nodes\n",
    "- Graph 1: 2 nodes\n",
    "- Graph 2: 4 nodes\n",
    "\n",
    "Nodes:  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "Graphs: [0, 0, 0, 1, 1, 2, 2, 2, 2]\n",
    "\n",
    "Graph 0:   ●---●---●      Batch = [0, 0, 0]\n",
    "\n",
    "Graph 1:   ●---●          Batch = [1, 1]\n",
    "\n",
    "Graph 2:   ●---●---●---●  Batch = [2, 2, 2, 2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check list slicing\n",
    "my_list = [1, 2, 3, 4]\n",
    "new_list = my_list[0 : 2]\n",
    "new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check `torch.block_diag`\n",
    "A_ = torch.tensor([[0, 1], [1, 0]])\n",
    "B_ = torch.tensor([[3, 4, 5], [6, 7, 8]])\n",
    "C_ = torch.tensor(7)\n",
    "D_ = torch.tensor([1, 2, 3])\n",
    "E_ = torch.tensor([[4], [5], [6]])\n",
    "torch.block_diag(A_, B_, C_, D_, E_) # Note block_diag expects separate tensors and that's why later we use unpacking operator *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.1416, 3.1416, 3.1416, 3.1416, 3.1416])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check `torch.full`\n",
    "tensor_a = torch.full((5,), 3.141592)\n",
    "tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "54KkQeG8wL73"
   },
   "outputs": [],
   "source": [
    "def graph_mini_batch(adj_matrix_list,  x_list,  y_list, batch_size=64):\n",
    "     # Implement the function here\n",
    "     num_graphs = len(adj_matrix_list)\n",
    "     for i in range(0, num_graphs, batch_size):\n",
    "        # Get the batch\n",
    "        A_batch = adj_matrix_list[i : i + batch_size]\n",
    "        X_batch = x_list[i : i + batch_size]\n",
    "        Y_batch = y_list[i : i + batch_size]\n",
    "        \n",
    "        # Compute the batched adjacency matrix (block-diagonal)\n",
    "        A_B = torch.block_diag(*A_batch) # Use unpacking operator * to unpact list into tensors\n",
    "        \n",
    "        # Concatenate node features and labels\n",
    "        X_B = torch.cat(X_batch, dim=0) # Concatenate them vertically\n",
    "        Y_B = torch.cat(Y_batch, dim=0)\n",
    "        \n",
    "        # Create the Batch vector\n",
    "        batch_vector = torch.cat([\n",
    "            torch.full((x.size(0),), idx, dtype=torch.long)  # Creates [idx, idx, ..., idx]\n",
    "            for idx, x in enumerate(X_batch)\n",
    "        ])\n",
    "        \n",
    "        # Yield the batch\n",
    "        yield A_B, X_B, Y_B, batch_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dpZQz5Q-zBek"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([585, 585])\n",
      "torch.Size([585, 7])\n",
      "torch.Size([32])\n",
      "torch.Size([585])\n",
      "torch.Size([583, 583])\n",
      "torch.Size([583, 7])\n",
      "torch.Size([32])\n",
      "torch.Size([583])\n",
      "torch.Size([589, 589])\n",
      "torch.Size([589, 7])\n",
      "torch.Size([32])\n",
      "torch.Size([589])\n",
      "torch.Size([590, 590])\n",
      "torch.Size([590, 7])\n",
      "torch.Size([32])\n",
      "torch.Size([590])\n",
      "torch.Size([498, 498])\n",
      "torch.Size([498, 7])\n",
      "torch.Size([32])\n",
      "torch.Size([498])\n",
      "torch.Size([526, 526])\n",
      "torch.Size([526, 7])\n",
      "torch.Size([28])\n",
      "torch.Size([526])\n"
     ]
    }
   ],
   "source": [
    "# Test your batch function here\n",
    "for adj_matrix, x, y, batch in graph_mini_batch(A, X, Y, batch_size):\n",
    "     print(adj_matrix.size())\n",
    "     print(x.size())\n",
    "     print(y.size())\n",
    "     print(batch.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7k4UptPbCf9"
   },
   "source": [
    "## Task 2\n",
    "Define a function `global_sum_pool` which takes a batch of node features and a Batch vector mapping each node to its respective graph in the batch, and outputs a batch of graph representation vectors by summing all node features in a graph.\n",
    "\n",
    "**Hints:** You are allowed to use the function scatter from torch_scatter library. See [here](https://pytorch-scatter.readthedocs.io/en/latest/functions/scatter.html#torch_scatter.scatter) for a detailed introduction about the usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "i6YFBsNrbdZr"
   },
   "outputs": [],
   "source": [
    "def global_sum_pool(x, batch):\n",
    "   # Implement the function here\n",
    "   # Use scatter to sum node features for each graph\n",
    "   \"\"\"\n",
    "    Perform global sum pooling on a batch of graphs.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Node feature matrix of shape (N, F),\n",
    "                          where N is the number of nodes in a batch, and F is the feature dimension.\n",
    "        batch (torch.Tensor): Batch vector of shape (N,), mapping nodes to graphs.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Graph-level representation matrix of shape (B, F),\n",
    "                      where B is the number of graphs in the batch.\n",
    "    \"\"\"\n",
    "   graph_representations = scatter(x, batch, dim=0, reduce='sum')\n",
    "   return graph_representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ABuRS36DVn5q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 7])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([32, 7])\n",
      "torch.Size([28, 7])\n"
     ]
    }
   ],
   "source": [
    "# Test your pooling function, assuming you are given a mini-batch of node features and a batch vector\n",
    "# Note that 5 * 32 + 28 = 188\n",
    "for _, x, _ , batch in graph_mini_batch(A, X, Y, batch_size):\n",
    "      sum_graph_rep =  global_sum_pool(x, batch)\n",
    "      print(sum_graph_rep.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more detailed explanation of the above `global_sum_pool` function to illustrate the idea.\n",
    "\n",
    "For the input:\n",
    "- Graph 0: Nodes $0,1 \\rightarrow$ Sum features: $[1+4,2+5,3+6]=[5,7,9]$\n",
    "- Graph 1: Nodes 2, $3 \\rightarrow$ Sum features: $[7+10,8+11,9+12]=[17,19,21]$\n",
    "- Graph 2: Node $4 \\rightarrow$ Features remain unchanged: $[13,14,15]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Representations:\n",
      " tensor([[ 5.,  7.,  9.],\n",
      "        [17., 19., 21.],\n",
      "        [13., 14., 15.]])\n"
     ]
    }
   ],
   "source": [
    "# Example node features (5 nodes, 3 features each)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0],\n",
    "                  [7.0, 8.0, 9.0],\n",
    "                  [10.0, 11.0, 12.0],\n",
    "                  [13.0, 14.0, 15.0]])\n",
    "\n",
    "# Example batch vector (5 nodes belonging to 3 graphs)\n",
    "batch = torch.tensor([0, 0, 1, 1, 2])  # Graph 0: 2 nodes, Graph 1: 2 nodes, Graph 2: 1 node\n",
    "\n",
    "# Perform global sum pooling\n",
    "graph_representations = global_sum_pool(x, batch)\n",
    "\n",
    "print(\"Graph Representations:\\n\", graph_representations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_BV3iHA9Gaw"
   },
   "source": [
    "# Model\n",
    "We now implement a GNN model, `GIN`, which is used to do the graph classification. **DO NOT** change the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "UUQGCU9-YQVP"
   },
   "outputs": [],
   "source": [
    "class GINConv(MessagePassing):\n",
    "    def __init__(self, emb_dim):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "        '''\n",
    "        super(GINConv, self).__init__(aggr = \"add\")\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(torch.nn.Linear(emb_dim, 2*emb_dim), torch.nn.BatchNorm1d(2*emb_dim), torch.nn.ReLU(), torch.nn.Linear(2*emb_dim, emb_dim))\n",
    "        self.eps = torch.nn.Parameter(torch.Tensor([0]))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        out = self.mlp((1 + self.eps) *x + self.propagate(edge_index, x=x))\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j):\n",
    "        return F.relu(x_j)\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "\n",
    "### GNN to generate node embedding\n",
    "class GIN(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        node representations\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layer, emb_dim, hidden_dim, drop_ratio = 0, JK = \"last\", residual = False):\n",
    "        '''\n",
    "            emb_dim (int): node embedding dimensionality\n",
    "            num_layer (int): number of GNN message passing layers\n",
    "        '''\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.JK = JK\n",
    "        ### add residual connection or not\n",
    "        self.residual = residual\n",
    "        if self.num_layer < 2:\n",
    "            raise ValueError(\"Number of GNN layers must be greater than 1.\")\n",
    "\n",
    "        self.embed = torch.nn.Linear(emb_dim, hidden_dim)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "        for layer in range(num_layer):\n",
    "            self.convs.append(GINConv(hidden_dim))\n",
    "            self.batch_norms.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h_list = [self.embed(x)]\n",
    "        for layer in range(self.num_layer):\n",
    "            h = self.convs[layer](h_list[layer], edge_index)\n",
    "            h = self.batch_norms[layer](h)\n",
    "            if layer == self.num_layer - 1:\n",
    "                #remove relu for the last layer\n",
    "                h = F.dropout(h, self.drop_ratio, training = self.training)\n",
    "            else:\n",
    "                h = F.dropout(F.relu(h), self.drop_ratio, training = self.training)\n",
    "\n",
    "            if self.residual:\n",
    "                h += h_list[layer]\n",
    "\n",
    "            h_list.append(h)\n",
    "        ### Different implementations of Jk-concat\n",
    "        if self.JK == \"last\":\n",
    "            node_representation = h_list[-1]\n",
    "        elif self.JK == \"sum\":\n",
    "            node_representation = 0\n",
    "            for layer in range(self.num_layer + 1):\n",
    "                node_representation += h_list[layer]\n",
    "\n",
    "        return node_representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIrzU7d_-K1L"
   },
   "source": [
    "### Define the hyperparameters we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uATxaDKL-QzC"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"input_features\": 7,\n",
    "    \"hidden_features\": 50,\n",
    "    \"num_layers\": 3,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 0,\n",
    "    \"num_epochs\": 100,\n",
    "    \"num_classes\": 2,\n",
    "    \"batch_size\": 32\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dict value\n",
    "params[\"num_layers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sX7kDGkY-jdk"
   },
   "source": [
    "Now, we come to the most exciting part, that is, training and evaluating the model based on the graph mini-batch and the graph pooling functions you implemented above.\n",
    "\n",
    "By checking the training time,  you can see the power of the graph batching by changing the batch_size, i.e., from 1 to 64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "64Mw0rgPeiEA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, loss=1.972123384475708, accuracy=0.664893627166748\n",
      "epoch=10, loss=0.1613469272851944, accuracy=0.7765957713127136\n",
      "epoch=20, loss=0.07435809075832367, accuracy=0.9095744490623474\n",
      "epoch=30, loss=0.03826948627829552, accuracy=0.9095744490623474\n",
      "epoch=40, loss=0.021557671949267387, accuracy=0.936170220375061\n",
      "epoch=50, loss=0.013883644714951515, accuracy=0.9308510422706604\n",
      "epoch=60, loss=0.009876693598926067, accuracy=0.9308510422706604\n",
      "epoch=70, loss=0.007313788402825594, accuracy=0.9308510422706604\n",
      "epoch=80, loss=0.005458567291498184, accuracy=0.9308510422706604\n",
      "epoch=90, loss=0.004191061481833458, accuracy=0.936170220375061\n",
      "time=9.226132869720459\n"
     ]
    }
   ],
   "source": [
    "begin_time = time.time()\n",
    "\n",
    "model = GIN(params[\"num_layers\"], params[\"input_features\"], params[\"hidden_features\"])\n",
    "pooling = global_sum_pool\n",
    "graph_pred_linear = torch.nn.Linear(params[\"hidden_features\"], params[\"num_classes\"])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model_param_group = [{\"params\": model.parameters(), \"lr\": params[\"learning_rate\"]}]\n",
    "if graph_pred_linear is not None:\n",
    "        model_param_group.append(\n",
    "            {\"params\": graph_pred_linear.parameters(), \"lr\": params[\"learning_rate\"]}\n",
    "        )\n",
    "\n",
    "optimizer = torch.optim.AdamW(model_param_group,\n",
    "                                lr=params[\"learning_rate\"],\n",
    "                                weight_decay=params[\"weight_decay\"])\n",
    "for epoch in range(params[\"num_epochs\"]):\n",
    "  model.train()\n",
    "  for adj_matrix, x, y, batch in graph_mini_batch(A, X, Y, params[\"batch_size\"]):\n",
    "      optimizer.zero_grad()\n",
    "      edge_index = U.dense_to_sparse(adj_matrix)[0]\n",
    "      nodes = model(x, edge_index)\n",
    "      graph_reps = pooling(nodes, batch)\n",
    "      pred = graph_pred_linear(graph_reps)\n",
    "      loss = loss_fn(pred, y)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "  if epoch % 10 == 0:\n",
    "      model.eval()\n",
    "      correct = 0\n",
    "      total_num = 0\n",
    "      for adj_matrix, x, y, batch in graph_mini_batch(A, X, Y, params[\"batch_size\"]):\n",
    "        optimizer.zero_grad()\n",
    "        edge_index = U.dense_to_sparse(adj_matrix)[0]\n",
    "        nodes = model(x, edge_index)\n",
    "        graph_reps = pooling(nodes, batch)\n",
    "        pred = graph_pred_linear(graph_reps)\n",
    "        correct += (pred.argmax(dim=-1) == y).sum()\n",
    "        total_num += len(y)\n",
    "      print(\"epoch={}, loss={}, accuracy={}\".format(epoch, loss.item(), correct/total_num))\n",
    "print(\"time={}\".format(time.time()-begin_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieNyeHvcVR9D"
   },
   "source": [
    "### Task 3 (Optional)\n",
    "This is an optional task. You are expected to implement 10-Fold Cross-validation. The general procedure is as follows:\n",
    "\n",
    "1. Shuffle the dataset randomly.\n",
    "2. Split the dataset into $k$ groups.\n",
    "For each unique group:\n",
    "  - Take the group as a hold out or test data set\n",
    "  - Take the remaining groups as a training data set\n",
    "  - Fit a model on the training set and evaluate it on the test set.\n",
    "  Retain the evaluation score and discard the model\n",
    "  - Summarize the skill of the model using the sample of model evaluation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indices: [5 6 7 8 9]\n",
      "Test indices: [0 1 2 3 4]\n",
      "Train indices: [0 1 2 3 4]\n",
      "Test indices: [5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "# Check KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Initialize random seed and dataset\n",
    "np.random.seed(42)\n",
    "number = np.random.permutation(10)\n",
    "\n",
    "# Set up KFold\n",
    "kf = KFold(n_splits=2, shuffle=False)\n",
    "\n",
    "# Iterate through the splits and print training/test indices\n",
    "for train_index, test_index in kf.split(number):\n",
    "    print(\"Train indices:\", train_index)\n",
    "    print(\"Test indices:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "VeZtBHmhhtX4"
   },
   "outputs": [],
   "source": [
    "# implement your solutions here\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def cross_validation(dataset, model_class, params, k=10):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: A tuple of (A_list, X_list, Y_list) representing adjacency matrices, node features, and labels.\n",
    "        model_class: The model class (e.g., GIN) to be trained.\n",
    "        params: A dictionary containing hyperparameters for the model.\n",
    "        k: Number of folds (default: 10).\n",
    "        \n",
    "    Returns:\n",
    "        mean_score: Mean evaluation score across folds.\n",
    "        std_score: Standard deviation of evaluation scores across folds.\n",
    "    \"\"\"\n",
    "    A_list, X_list, Y_list = dataset\n",
    "    n_graphs = len(A_list)\n",
    "\n",
    "    # Ensure reproducibility by seeding\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.permutation(n_graphs) # A numpy array containing shuffled indices\n",
    "\n",
    "    # Prepare k-fold splitting\n",
    "    kf = KFold(n_splits=k, shuffle=False) # kf is an instance of KFold class\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(indices)):\n",
    "        print(f\"Processing Fold {fold + 1}/{k}...\")\n",
    "\n",
    "        # Split dataset into training and test sets\n",
    "        train_A = [A_list[i] for i in train_idx]\n",
    "        train_X = [X_list[i] for i in train_idx]\n",
    "        train_Y = [Y_list[i] for i in train_idx]\n",
    "\n",
    "        test_A = [A_list[i] for i in test_idx]\n",
    "        test_X = [X_list[i] for i in test_idx]\n",
    "        test_Y = [Y_list[i] for i in test_idx]\n",
    "\n",
    "        # Initialize model, optimizer, and loss function\n",
    "        model = model_class(params[\"num_layers\"], params[\"input_features\"], params[\"hidden_features\"])\n",
    "        pooling = global_sum_pool\n",
    "        graph_pred_linear = torch.nn.Linear(params[\"hidden_features\"], params[\"num_classes\"])\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        model_param_group = [{\"params\": model.parameters(), \"lr\": params[\"learning_rate\"]}]\n",
    "        model_param_group.append({\"params\": graph_pred_linear.parameters(), \"lr\": params[\"learning_rate\"]})\n",
    "\n",
    "        optimizer = torch.optim.AdamW(model_param_group, \n",
    "                                      lr=params[\"learning_rate\"], \n",
    "                                      weight_decay=params[\"weight_decay\"])\n",
    "\n",
    "        # Train model\n",
    "        for epoch in range(params[\"num_epochs\"]):\n",
    "            model.train()\n",
    "            for adj_matrix, x, y, batch in graph_mini_batch(train_A, train_X, train_Y, params[\"batch_size\"]):\n",
    "                optimizer.zero_grad()\n",
    "                edge_index = U.dense_to_sparse(adj_matrix)[0]\n",
    "                nodes = model(x, edge_index)\n",
    "                graph_reps = pooling(nodes, batch)\n",
    "                pred = graph_pred_linear(graph_reps)\n",
    "                loss = loss_fn(pred, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate model on the test set\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total_num = 0\n",
    "        with torch.no_grad():\n",
    "            for adj_matrix, x, y, batch in graph_mini_batch(test_A, test_X, test_Y, params[\"batch_size\"]):\n",
    "                edge_index = U.dense_to_sparse(adj_matrix)[0]\n",
    "                nodes = model(x, edge_index)\n",
    "                graph_reps = pooling(nodes, batch)\n",
    "                pred = graph_pred_linear(graph_reps)\n",
    "                correct += (pred.argmax(dim=-1) == y).sum().item()\n",
    "                total_num += len(y)\n",
    "\n",
    "        accuracy = correct / total_num\n",
    "        print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "        scores.append(accuracy)\n",
    "\n",
    "    # Compute mean and standard deviation of scores\n",
    "    mean_score = np.mean(scores)\n",
    "    std_score = np.std(scores)\n",
    "\n",
    "    print(f\"\\nCross-Validation Results: Mean Accuracy = {mean_score:.4f}, Std = {std_score:.4f}\")\n",
    "    return mean_score, std_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10...\n",
      "Fold 1 Accuracy: 0.8947\n",
      "Processing Fold 2/10...\n",
      "Fold 2 Accuracy: 0.7895\n",
      "Processing Fold 3/10...\n",
      "Fold 3 Accuracy: 0.7895\n",
      "Processing Fold 4/10...\n",
      "Fold 4 Accuracy: 0.7895\n",
      "Processing Fold 5/10...\n",
      "Fold 5 Accuracy: 0.8947\n",
      "Processing Fold 6/10...\n",
      "Fold 6 Accuracy: 0.7368\n",
      "Processing Fold 7/10...\n",
      "Fold 7 Accuracy: 0.9474\n",
      "Processing Fold 8/10...\n",
      "Fold 8 Accuracy: 0.6842\n",
      "Processing Fold 9/10...\n",
      "Fold 9 Accuracy: 0.9444\n",
      "Processing Fold 10/10...\n",
      "Fold 10 Accuracy: 0.7778\n",
      "\n",
      "Cross-Validation Results: Mean Accuracy = 0.8249, Std = 0.0852\n"
     ]
    }
   ],
   "source": [
    "dataset = (A, X, Y) \n",
    "mean_acc, std_acc = cross_validation(dataset, GIN, params, k=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "GRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
