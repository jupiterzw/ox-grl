{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "Welcome to the first practical for Graph Representation Learning. In this practical, we will be covering the content of the lectures about TransE.\n",
        "\n",
        "We will be using [PyTorch](https://pytorch.org/docs/stable/index.html) to implement TransE from scratch, building it up piece by piece.\n",
        "\n",
        "The main goal of this practical is to create a working implementation of TransE. There are also two optional parts: *filtered negative sampling* and *RotatE*.\n",
        "\n",
        "The notebook is divided into sections, each of which comes with complete or partially completed code. Before each snippet of code there will be a description of what we are about to implement. The sections of code you need to complete are marked as **Tasks**. The majority of the length of this practical comes from code already written for you, so don't panic at the apparent length! There are only 8 tasks for you to complete.\n",
        "\n",
        "Please ensure that you operate within the framework given in the notebook and bring any questions you may have to the practical demonstrators. We suggest that you **DO NOT** edit code that is a part of the framework, since this will make it more difficult for demonstrators to assist if your code is broken.\n",
        "\n",
        "Since we are working in a Jupyter Notebook, the code is very interactive. When you're stuck on something, try adding a new block of code below what you're working on and using it to debug your code. If you are new to Jupyter Notebooks, see [here](https://www.youtube.com/watch?v=inN8seMm7UI&ab_channel=TensorFlow) for a brief introduction video. If you are using Google Colab (which we recommend doing), please ensure you have changed the runtime type to use a GPU, as it will make your code run much faster."
      ],
      "metadata": {
        "id": "sKAJiNwUvqZM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5IOc-BMFfQw"
      },
      "source": [
        "# Imports\n",
        "\n",
        "Run the following blocks of code to install and import and the necessary python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kODAA4k06_PS"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pykeen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HN8bYYUf7FQw"
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pykeen\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils import data as torch_data\n",
        "from sklearn.metrics import average_precision_score\n",
        "from pykeen.datasets import Nations\n",
        "from typing import List, Tuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJmiuJqw78RR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySL6dn4NBQQe"
      },
      "source": [
        "## Loading Nations from `pykeen`\n",
        "We will use `pykeen` to load the `Nations` dataset, which is a small knowledge graph with 14 entities, 55 relations, and 1992 triples describing countries and their political relationships.\n",
        "\n",
        "We first define a function to convert the `pykeen` datasets to lists of triples. We then create 3 lists of triples: one for training, one for validation, and one for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr3eij-8Ejyg"
      },
      "outputs": [],
      "source": [
        "def create_triples_from_pykeen_dataset(dataset: pykeen.triples.triples_factory.TriplesFactory):\n",
        "    slcwa_instances=dataset.create_slcwa_instances(\n",
        "        batch_size=1,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    positive_dataset = [tuple(batch.positives[0].tolist()) for batch in slcwa_instances]\n",
        "    return positive_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZfQ66zYEa71"
      },
      "outputs": [],
      "source": [
        "dataset = Nations()\n",
        "train_triples = create_triples_from_pykeen_dataset(dataset.training)\n",
        "valid_triples = create_triples_from_pykeen_dataset(dataset.validation)\n",
        "test_triples = create_triples_from_pykeen_dataset(dataset.testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 1\n",
        "Define a function `id_triple_as_labels` that takes in a triple of entity/relation ids and returns a triple of labels.\n",
        "\n",
        "*Hint: use the dictionaries `id2entity` and `id2relation` provided below*"
      ],
      "metadata": {
        "id": "GtqqZDPjzRXH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrHfuTC7o4AP"
      },
      "outputs": [],
      "source": [
        "id2entity = dataset.training.entity_id_to_label\n",
        "id2relation = dataset.training.relation_id_to_label\n",
        "\n",
        "### BEGIN SOLUTION\n",
        "def id_triple_as_labels(triple: Tuple):\n",
        "    return ...\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then run the following cell to see some of the facts from the **Nations** dataset."
      ],
      "metadata": {
        "id": "76Jeu4x-z8Ti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUcWeK4fo-U8"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    print(train_triples[i])\n",
        "    print(id_triple_as_labels(train_triples[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Negative Sampling\n",
        "Simply training on positive facts will not suffice, since the model will then learn to just maximise the similarity measure for every possible fact in the database. Thus, for each positive fact, we need to sample a set of corrupted facts."
      ],
      "metadata": {
        "id": "NSS5FB7K9-c-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 2\n",
        "First, define a function `get_corrupted_entities` that takes in a single positive sample and returns an array of corrupted entities. The positive sample is a tuple of integers, representing the IDs of the entities/relation.\n",
        "\n",
        "The function should also take in `train_dataset`, as you will need to access `train_dataset.negative_sample_size` (the number of corrupted entities to sample), `train_dataset.nentity` (the number of entities in the knowledge graph), and possibly `train_dataset.true_head` / `train_dataset.true_tail` (dictionaries providing the set of all known true triples for the given head / tail). The keys for the train_dataset.true_head dictionary are tuples of the form (relation, tail), and the keys for train_dataset.true_tail are tuples of the form (head, relation). The full definition of `TrainDataset` can be found further below if you need to refer to it.\n",
        "\n",
        "The function should also take in `mode`, which, if set to `mode == 'head'`, indicates that the head entity should be corrupted, and likewise the tail entity if `mode == 'tail'`. Your output should be a numpy array with shape `[train_dataset.negative_sample_size]`.\n",
        "\n",
        "Note: as an optional extra, you can sample corrupted entities such that the resulting triples are not known to be true in the knowledge graph. If you get stuck on this, first implement a simpler solution first and then come back to it later."
      ],
      "metadata": {
        "id": "oTbungL01h45"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghDlxhhaRAqQ"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "def get_corrupted_entities(positive_sample: Tuple, train_dataset: Dataset, mode: str) -> np.ndarray:\n",
        "    head, relation, tail = positive_sample\n",
        "    ...\n",
        "    return negative_sample\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 3\n",
        "Next, we can create the negative samples using the corrupted entities. Define a function `get_negative_sample` which takes in a positive sample and corrupted head/tail entities, which will be created by the `get_corrupted_entities` function we defined above (do not call to the above function, the corrupted entities will be passed in as arguments).\n",
        "\n",
        "The argument `positive_sample` is a tuple `(head, relation, tail)`. The arguments `corrupted_head_entities` and `corrupted_tail_entities` should have shape `[negative_sample_size]` each.\n",
        "\n",
        "Your function should return a `numpy` array with shape `[2*negative_sample_size, 3]`, by combining the corrupted entities with the positive sample."
      ],
      "metadata": {
        "id": "OKlY_jD_5SLE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def get_negative_sample(positive_sample, corrupted_head_entities, corrupted_tail_entities):\n",
        "    corrupted_head_samples = np.zeros((corrupted_head_entities.shape[0], 3))\n",
        "    corrupted_head_samples[:, 0] = corrupted_head_entities\n",
        "    corrupted_head_samples[:, 1:3] = positive_sample[1:3]\n",
        "\n",
        "    corrupted_tail_samples = ...\n",
        "\n",
        "    return np.concatenate((corrupted_head_samples, corrupted_tail_samples), axis=0)\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "fDbo_35aRC1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Dataset\n",
        "Now that we have written the functions we need to perform our negative sampling, let's combine everything together to create our `torch` training dataset.\n",
        "\n",
        "Notice that in the `__get_item__` function, we convert the samples to `torch` tensors before we return them. Up to this point, we have been working with `numpy` arrays; `torch` tensors have the same structure, but are optimised to run on the `GPU` and can also track gradients to be used for optimising parameters."
      ],
      "metadata": {
        "id": "st_ZunLh-yXD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YPVX9Y_7h-g"
      },
      "outputs": [],
      "source": [
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, triples, nentity, nrelation, negative_sample_size):\n",
        "        self.len = len(triples)\n",
        "        self.triples = triples # all training triples\n",
        "        self.triple_set = set(triples) # unique triples\n",
        "        self.nentity = nentity # number of entities in the knowledge graph\n",
        "        self.nrelation = nrelation # number of relations in the knowledge graph\n",
        "        self.negative_sample_size = negative_sample_size // 2 # Half from heads, half from tails\n",
        "\n",
        "        # known triples for the given heads / tails\n",
        "        self.true_head, self.true_tail = self.get_true_head_and_tail(self.triples)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "        Get an item from the Dataset.\n",
        "        '''\n",
        "        # Fetch a positive sample\n",
        "        positive_sample = self.triples[idx]\n",
        "\n",
        "        # Sample corrupted head and tail entities\n",
        "        corrupted_head_entities = get_corrupted_entities(positive_sample, self, 'head')\n",
        "        corrupted_tail_entities = get_corrupted_entities(positive_sample, self, 'tail')\n",
        "\n",
        "        # Create the negative sample\n",
        "        negative_sample = get_negative_sample(positive_sample, corrupted_head_entities, corrupted_tail_entities)\n",
        "\n",
        "        # Convert samples to torch tensors\n",
        "        negative_sample = torch.LongTensor(negative_sample)\n",
        "        positive_sample = torch.LongTensor(positive_sample)\n",
        "\n",
        "        return positive_sample, negative_sample\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
        "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
        "        return positive_sample, negative_sample\n",
        "\n",
        "    @staticmethod\n",
        "    def get_true_head_and_tail(triples):\n",
        "        '''\n",
        "        Build a dictionary of true triples that will\n",
        "        be used to filter these true triples for negative sampling\n",
        "        '''\n",
        "\n",
        "        true_head = {}\n",
        "        true_tail = {}\n",
        "\n",
        "        for head, relation, tail in triples:\n",
        "            if (head, relation) not in true_tail:\n",
        "                true_tail[(head, relation)] = []\n",
        "            true_tail[(head, relation)].append(tail)\n",
        "            if (relation, tail) not in true_head:\n",
        "                true_head[(relation, tail)] = []\n",
        "            true_head[(relation, tail)].append(head)\n",
        "\n",
        "        for relation, tail in true_head:\n",
        "            true_head[(relation, tail)] = np.array(list(set(true_head[(relation, tail)])))\n",
        "        for head, relation in true_tail:\n",
        "            true_tail[(head, relation)] = np.array(list(set(true_tail[(head, relation)])))\n",
        "\n",
        "        return true_head, true_tail\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset\n",
        "We will use a seperate dataset for our testing, with the batch size always set to 1. The test dataset will have two modes: `'head-batch'` and `'tail-batch'`. In the first mode, the dataset should return a positive sample and a list of all possible heads, and similarly for the second mode.\n",
        "\n",
        "Since we are doing filtered evaluation, we do not want triples which are known to be true to affect the ranking. Thus, we will filter the heads / tails out of our samples that yield triples which are known to be true."
      ],
      "metadata": {
        "id": "Q8pygs4q-WQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 4\n",
        "Write a function `get_filtered_test_sample` that takes in a positive triple (`head, relation, tail`) and the `test_dataset` (which can be found further below). The function should return a list of size `test_dataset.nentity`, where each element is the ID of an entity. It is important for the downstream ranking that we ensure the list is this size.\n",
        "\n",
        "To filter the sample, if for some entity `x`, `(x, relation, tail)` already appears in the set of known triples, instead replace it with `head`. The set of known triples can be accessed by `test_dataset.triple_set`. You will need to check the value of `test_dataset.mode` and return a list of entities accordingly."
      ],
      "metadata": {
        "id": "m_GCF-QCko82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def get_filtered_test_sample(head, relation, tail, test_dataset: Dataset) -> List:\n",
        "    if test_dataset.mode == 'head-batch':\n",
        "        ...\n",
        "    elif test_dataset.mode == 'tail-batch':\n",
        "        ...\n",
        "    else:\n",
        "        raise ValueError('negative batch mode %s not supported' % test_dataset.mode)\n",
        "    return filtered_sample\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "PSD3fMg7eh70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our filtered test sampling function to define our test dataset."
      ],
      "metadata": {
        "id": "xOV-9ixToFIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, triples, all_true_triples, nentity, nrelation, mode):\n",
        "        self.len = len(triples)\n",
        "        self.triple_set = set(all_true_triples) # set of all known true triples\n",
        "        self.triples = triples # test triples\n",
        "        self.nentity = nentity\n",
        "        self.nrelation = nrelation\n",
        "        self.mode = mode # 'head-batch' or 'tail-batch'\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        head, relation, tail = self.triples[idx] # fetch a positive sample from the test triples\n",
        "\n",
        "        # get the filtered sample using the function we defined\n",
        "        filtered_sample = get_filtered_test_sample(head, relation, tail, self)\n",
        "\n",
        "        # convert to torch tensors\n",
        "        filtered_sample = torch.LongTensor(filtered_sample)\n",
        "        positive_sample = torch.LongTensor((head, relation, tail))\n",
        "\n",
        "        return positive_sample, filtered_sample, self.mode\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
        "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
        "        mode = data[0][2]\n",
        "        return positive_sample, negative_sample, mode"
      ],
      "metadata": {
        "id": "6iXpggcLKjZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Iterator\n",
        "As a final step towards constructing our datasets, we define a class that allows us to convert `torch` dataloaders into python iterators, which will make it simpler for us to define training and testing step functions."
      ],
      "metadata": {
        "id": "NkNwwu5W-qj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneShotIterator:\n",
        "    def __init__(self, dataloader):\n",
        "        self.iterator = self.one_shot_iterator(dataloader)\n",
        "\n",
        "    def __next__(self):\n",
        "        return next(self.iterator)\n",
        "\n",
        "    @staticmethod\n",
        "    def one_shot_iterator(dataloader):\n",
        "        '''\n",
        "        Transform a PyTorch Dataloader into python iterator\n",
        "        '''\n",
        "        while True:\n",
        "            for data in dataloader:\n",
        "                yield data"
      ],
      "metadata": {
        "id": "XaQf4pHj-mCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will create the actual train and test datasets in code further below, but here follows some code for constructing them, in case you would like to use them for debugging."
      ],
      "metadata": {
        "id": "c3L96bkppgXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make the train dataloader\n",
        "train_dataloader = DataLoader(\n",
        "    TrainDataset(train_triples,\n",
        "                 14, # nentity\n",
        "                 55, # nrelation\n",
        "                 128), # negative sampling size\n",
        "    batch_size=500,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        "    collate_fn=TrainDataset.collate_fn\n",
        ")\n",
        "# convert to an iterator\n",
        "train_iterator = OneShotIterator(train_dataloader)\n",
        "# get a sample from the train iterator\n",
        "positive_sample, negative_sample = next(train_iterator)"
      ],
      "metadata": {
        "id": "tJmSoj1DpoFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, the following output should be `torch.Size([500, 128, 3])`."
      ],
      "metadata": {
        "id": "8NJXmueor5HB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_sample.size()"
      ],
      "metadata": {
        "id": "z2UO0M1YqlhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the test dataloader\n",
        "known_true_triples = train_triples + valid_triples + test_triples\n",
        "test_dataloader_head = DataLoader(\n",
        "    TestDataset(\n",
        "        test_triples,\n",
        "        known_true_triples,\n",
        "        14, # nentity\n",
        "        55, # nrelation\n",
        "        'head-batch'\n",
        "    ),\n",
        "    batch_size=1,\n",
        "    num_workers=1,\n",
        "    collate_fn=TestDataset.collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "thhn9ZCOqj_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your code is correct, the following output should be `torch.Size([1, 14])`."
      ],
      "metadata": {
        "id": "q_sbcRuXryAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for positive_sample, negative_sample, mode in test_dataloader_head:\n",
        "    print(negative_sample.size())\n",
        "    break"
      ],
      "metadata": {
        "id": "jl-ZEaM7rDaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMHxh01176Aw"
      },
      "source": [
        "# Model\n",
        "We now define our model, `KGEModel`. It is built in such a way that we can implement different dissimilarity measures within it.\n",
        "\n",
        "From here onwards, we will be working with `torch` tensors instead of `numpy` arrays, so make sure you are using `torch` operations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Initialisation\n",
        "We will use `torch.nn.Parameter` to store our embeddings for entities and relations. We define a function `init_params` which initialises an embedding tensor of the given size and randomly samples values from the uniform distribution `[-embedding_range, embedding_range]`."
      ],
      "metadata": {
        "id": "ghEH_cnNDMpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_params(tensor_size: tuple, embedding_range: float) -> nn.Parameter:\n",
        "    embedding = nn.Parameter(torch.zeros(tensor_size))\n",
        "    nn.init.uniform_(\n",
        "        tensor=embedding,\n",
        "        a=-embedding_range,\n",
        "        b=embedding_range\n",
        "    )\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "ZlTLS7IhDL8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scoring Function\n",
        "Different KG embedding models use different dissimilarity measures (aka scoring functions). We will define one for **TransE** and optionally define one for **RotatE**."
      ],
      "metadata": {
        "id": "XRfGJezKpO6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 5\n",
        "Define a scoring function for **TransE** that takes in the head, relation, and tail, and returns a score for the triple. Each argument tensor has size `[batch_size, embedding_size]`. You may use either the $L_1$ or $L_2$ norm. Your output tensor should have size `[batch_size]`."
      ],
      "metadata": {
        "id": "iUV1Q2CItxlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def TransE(head, relation, tail):\n",
        "    ...\n",
        "    return score\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "POsujQKbpUzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6 (Optional)\n",
        "This is an optional task. You should get **TransE** working completely first and then come back to this.\n",
        "\n",
        "Define a scoring function for **RotatE**. The head and tail will have size `[batch_size, 2 * embedding_size]` to store both the real and imaginary parts of the entities. *Hint: you can use `torch.chunk()` to split the tensor into its real and imaginary components*.\n",
        "\n",
        "The relation will have size `[batch_size, embedding_size]`, representing the phase $\\theta$ of the relation. *Hint: The real and imaginary components of the relation can be computed with `torch.cos` and `torch.sin`*."
      ],
      "metadata": {
        "id": "gZgxQhEepZ8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def RotatE(head, relation, tail):\n",
        "    ...\n",
        "    return score\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "T_OU9yU_pbm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full Model Definition\n",
        "Now we can use our scoring function to define the model. Notice that in the `forward` function, we use `torch.index_select` to fetch the entity / relation embeddings from the their indices. `sample` is a tensor with the size `[batch_size, 3]`."
      ],
      "metadata": {
        "id": "yFFX3RsxpPaI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAfa7_U77o5L"
      },
      "outputs": [],
      "source": [
        "class KGEModel(nn.Module):\n",
        "    def __init__(self, model_name: str, nentity: int, nrelation: int, hidden_dim: int, gamma: float,\n",
        "                 double_entity_embedding: bool=False, double_relation_embedding: bool=False):\n",
        "        super(KGEModel, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.nentity = nentity\n",
        "        self.nrelation = nrelation\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.epsilon = 2.0\n",
        "\n",
        "        self.gamma = nn.Parameter(\n",
        "            torch.Tensor([gamma]),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        self.embedding_range = nn.Parameter(\n",
        "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        self.entity_dim = 2*hidden_dim if double_entity_embedding else hidden_dim\n",
        "        self.relation_dim = 2*hidden_dim if double_relation_embedding else hidden_dim\n",
        "\n",
        "        # Create entity and relation embeddings\n",
        "        self.entity_embedding = init_params(tensor_size=(nentity, self.entity_dim),\n",
        "                                             embedding_range=self.embedding_range.item())\n",
        "        self.relation_embedding = init_params(tensor_size=(nrelation, self.relation_dim),\n",
        "                                              embedding_range=self.embedding_range.item())\n",
        "\n",
        "        # This code supports easily adding new models like RotatE and ComplEx\n",
        "        # Do not forget to modify this line when you add a new model in the \"forward\" function\n",
        "        if model_name not in ['TransE', 'RotatE']:\n",
        "            raise ValueError('model %s not supported' % model_name)\n",
        "\n",
        "        if model_name == 'RotatE' and not double_entity_embedding:\n",
        "            raise ValueError('RotatE should use --double_entity_embedding')\n",
        "        if model_name == 'TransE' and double_entity_embedding:\n",
        "            raise ValueError('TransE should not use --double_entity_embedding')\n",
        "\n",
        "    def forward(self, sample):\n",
        "        '''\n",
        "        Forward function that calculate the score of a batch of triples.\n",
        "        Sample is a batch of triples.\n",
        "        '''\n",
        "        head = torch.index_select(\n",
        "            self.entity_embedding,\n",
        "            dim=0,\n",
        "            index=sample[:,0]\n",
        "        )\n",
        "\n",
        "        relation = torch.index_select(\n",
        "            self.relation_embedding,\n",
        "            dim=0,\n",
        "            index=sample[:,1]\n",
        "        )\n",
        "\n",
        "        tail = torch.index_select(\n",
        "            self.entity_embedding,\n",
        "            dim=0,\n",
        "            index=sample[:,2]\n",
        "        )\n",
        "\n",
        "        # Other models can be added here\n",
        "        dissimilarity_measure = {\n",
        "            'TransE': TransE,\n",
        "            'RotatE': RotatE,\n",
        "        }\n",
        "\n",
        "        if self.model_name in dissimilarity_measure:\n",
        "            score = dissimilarity_measure[self.model_name](head, relation, tail)\n",
        "        else:\n",
        "            raise ValueError('model %s not supported' % self.model_name)\n",
        "\n",
        "        return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "In this section, we will first write a function to compute the model loss given a set of positive and negative samples, and then use it to define a single training step for the model."
      ],
      "metadata": {
        "id": "MLu2TKQh2Su_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 7\n",
        "Before we can define a training step for our model, we must define a loss function for the model. We use negative sampling loss (from the **RotatE** paper), but without the self-adversarial parameter. Remember to refer to the lecture slides if you get stuck on this.\n",
        "\n",
        "Define a function `get_model_loss` which takes in the `KGEModel`, the positive sample, and the negative sample, and returns a tuple of the loss, the positive sample loss, and the negative sample loss.\n",
        "\n",
        "The positive sample will have size `[batch_size, 3]`, the negative sample will have size `[batch_size * negative_sampling_size, 3]`, and the margin $\\gamma$ can be accessed through `model.gamma`."
      ],
      "metadata": {
        "id": "i0B2CufDyMxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def get_model_loss(model: KGEModel, positive_sample: torch.tensor, negative_sample: torch.tensor) -> Tuple:\n",
        "    positive_sample_loss = ...\n",
        "    negative_sample_loss = ...\n",
        "\n",
        "    loss = positive_sample_loss + negative_sample_loss\n",
        "    return loss, positive_sample_loss, negative_sample_loss\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "7OcuBe6gnhT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define a single train step for the model."
      ],
      "metadata": {
        "id": "4wFfnDat0I_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model, optimizer, train_iterator, args):\n",
        "    '''\n",
        "    A single train step. Apply back-propation and return the loss\n",
        "    '''\n",
        "\n",
        "    model.train() # tell the torch model it's about to be trained\n",
        "\n",
        "    optimizer.zero_grad() # explicitly set gradients to 0 before starting backprop\n",
        "\n",
        "    positive_sample, negative_sample = next(train_iterator) # fetch samples from the dataset\n",
        "\n",
        "    # reshape the negative sample\n",
        "    # it will now have shape [batch_size * negative_sampling_size, 3]\n",
        "    negative_sample = torch.reshape(negative_sample, (-1, 3))\n",
        "\n",
        "    # move tensors to GPU\n",
        "    if args.cuda:\n",
        "        positive_sample = positive_sample.cuda()\n",
        "        negative_sample = negative_sample.cuda()\n",
        "\n",
        "    # compute the loss\n",
        "    loss, positive_sample_loss, negative_sample_loss = get_model_loss(model, positive_sample, negative_sample)\n",
        "\n",
        "    # apply loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    log = {\n",
        "        'positive_sample_loss': positive_sample_loss.item(),\n",
        "        'negative_sample_loss': negative_sample_loss.item(),\n",
        "        'loss': loss.item()\n",
        "    }\n",
        "\n",
        "    return log"
      ],
      "metadata": {
        "id": "XMIYGukH2UnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "In this section, we will first write a function to get the ranking of a positive entity compared to its corrupted counterparts, and then use that to define a single test step for the model."
      ],
      "metadata": {
        "id": "CWCLuazd2EZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task 8"
      ],
      "metadata": {
        "id": "rSsRpPyJ1V3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a function `get_ranking` which takes in entity scores and the index of the positive entity. The function should return an integer representing the rank of the score of the positive entity in relation to the other entities. Note that a rank of 1 represents having the *lowest* score.\n",
        "\n",
        "`entity_scores` has size `[nentity]`. Recall from the function we defined further above that we filtered out known true triples by replacing the corrupted heads with the actual head, so some of the entity scores may actually be that of the positive entity, even when they are not in the index of that entity.\n",
        "\n",
        "*Hint: torch.argsort will be very useful for this task*."
      ],
      "metadata": {
        "id": "bkDrEhHl3ES7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "def get_ranking(entity_scores: torch.tensor, positive_entity: int) -> int:\n",
        "    ...\n",
        "    return ranking\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "4Mn5PxEHuH5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now define a single test step for the model, using the ranking function to compute MRR, MR, and HITS@k metrics."
      ],
      "metadata": {
        "id": "JmlKohB21Y10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(model, test_triples, all_true_triples, args):\n",
        "    '''\n",
        "    Evaluate the model on test or valid datasets\n",
        "    '''\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    #Prepare dataloader for evaluation\n",
        "    test_dataloader_head = DataLoader(\n",
        "        TestDataset(\n",
        "            test_triples,\n",
        "            all_true_triples,\n",
        "            args.nentity,\n",
        "            args.nrelation,\n",
        "            'head-batch'\n",
        "        ),\n",
        "        batch_size=args.test_batch_size,\n",
        "        num_workers=max(1, args.cpu_num//2),\n",
        "        collate_fn=TestDataset.collate_fn\n",
        "    )\n",
        "\n",
        "    test_dataloader_tail = DataLoader(\n",
        "        TestDataset(\n",
        "            test_triples,\n",
        "            all_true_triples,\n",
        "            args.nentity,\n",
        "            args.nrelation,\n",
        "            'tail-batch'\n",
        "        ),\n",
        "        batch_size=args.test_batch_size,\n",
        "        num_workers=max(1, args.cpu_num//2),\n",
        "        collate_fn=TestDataset.collate_fn\n",
        "    )\n",
        "\n",
        "    test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
        "\n",
        "    logs = []\n",
        "\n",
        "    step = 0\n",
        "    total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
        "\n",
        "    # torch.no_grad() since we don't need to track gradients when we're testing\n",
        "    with torch.no_grad():\n",
        "        for test_dataset in test_dataset_list: # each of head / tail\n",
        "            for positive_sample, negative_sample, mode in test_dataset:\n",
        "                # take a sample from the test dataset\n",
        "                if args.cuda:\n",
        "                    positive_sample = positive_sample.cuda()\n",
        "                    negative_sample = negative_sample.cuda()\n",
        "\n",
        "                batch_size = positive_sample.size(0)\n",
        "                assert batch_size == 1, 'evaluation batch size must be set to 1'\n",
        "\n",
        "                # build the negative sample from the entities\n",
        "                # currently, negative_sample is just a list of entities\n",
        "                # [1, 14, 3] for Nations\n",
        "                built_negative_sample = torch.zeros((batch_size, negative_sample.size()[1], 3), dtype=int)\n",
        "                if args.cuda:\n",
        "                    built_negative_sample = built_negative_sample.cuda()\n",
        "\n",
        "                if mode == 'head-batch':\n",
        "                    built_negative_sample[:, :, 0] = negative_sample\n",
        "                    built_negative_sample[:, :, 1:3] = positive_sample[:, 1:3].unsqueeze(dim=1).expand((-1, built_negative_sample.size(1), -1))\n",
        "                else:\n",
        "                    built_negative_sample[:, :, 2] = negative_sample\n",
        "                    built_negative_sample[:, :, 0:2] = positive_sample[:, 0:2].unsqueeze(dim=1).expand((-1, built_negative_sample.size(1), -1))\n",
        "\n",
        "                # get the scores for each entity\n",
        "                negative_sample = built_negative_sample.reshape((-1, 3))\n",
        "                entity_scores = model(negative_sample)\n",
        "\n",
        "                # retrieve the positive entity\n",
        "                if mode == 'head-batch':\n",
        "                    positive_entity = positive_sample[:, 0].item()\n",
        "                elif mode == 'tail-batch':\n",
        "                    positive_entity = positive_sample[:, 2].item()\n",
        "                else:\n",
        "                    raise ValueError('mode %s not supported' % mode)\n",
        "\n",
        "                # get the ranking of the positive entity\n",
        "                ranking = get_ranking(entity_scores, positive_entity)\n",
        "\n",
        "                # compute and append logs\n",
        "                logs.append({\n",
        "                    'MRR': 1.0/ranking,\n",
        "                    'MR': float(ranking),\n",
        "                    'HITS@1': 1.0 if ranking <= 1 else 0.0,\n",
        "                    'HITS@3': 1.0 if ranking <= 3 else 0.0,\n",
        "                    'HITS@10': 1.0 if ranking <= 10 else 0.0,\n",
        "                })\n",
        "\n",
        "                if step % args.test_log_steps == 0:\n",
        "                    print('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
        "\n",
        "                step += 1\n",
        "\n",
        "    metrics = {}\n",
        "    for metric in logs[0].keys():\n",
        "        metrics[metric] = sum([log[metric] for log in logs])/len(logs)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "1muCky012KUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running\n",
        "To make the running of experiments easier, we will define several help functions."
      ],
      "metadata": {
        "id": "2y0CUaUWsepS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJkg1N0skfAp"
      },
      "source": [
        "## Arguments\n",
        "`argparse` is a very useful library for managing program arguments, particulary when executing from the command line. Default arguments are defined here. If you want to change argument values, do not do it in this block of code, rather change them in the arguments that are passed through to the program (see further below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5szUw0_khK8"
      },
      "outputs": [],
      "source": [
        "def parse_args(args=None):\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Training and Testing Knowledge Graph Embedding Models',\n",
        "        usage='train.py [<args>] [-h | --help]'\n",
        "    )\n",
        "\n",
        "    parser.add_argument('--cuda', action='store_true', help='use GPU')\n",
        "\n",
        "    parser.add_argument('--do_train', action='store_true')\n",
        "    parser.add_argument('--do_valid', action='store_true')\n",
        "    parser.add_argument('--do_test', action='store_true')\n",
        "    parser.add_argument('--evaluate_train', action='store_true', help='Evaluate on training data')\n",
        "\n",
        "    parser.add_argument('--model', default='TransE', type=str)\n",
        "    parser.add_argument('-de', '--double_entity_embedding', action='store_true')\n",
        "    parser.add_argument('-dr', '--double_relation_embedding', action='store_true')\n",
        "\n",
        "    parser.add_argument('-n', '--negative_sample_size', default=128, type=int)\n",
        "    parser.add_argument('-d', '--hidden_dim', default=100, type=int, help='Embedding size')\n",
        "    parser.add_argument('-g', '--gamma', default=2.0, type=float, help='Fixed margin parameter')\n",
        "    parser.add_argument('-b', '--batch_size', default=1024, type=int)\n",
        "    parser.add_argument('--test_batch_size', default=1, type=int, help='valid/test batch size (must be 1)')\n",
        "\n",
        "    parser.add_argument('-lr', '--learning_rate', default=0.0001, type=float)\n",
        "    parser.add_argument('-cpu', '--cpu_num', default=1, type=int)\n",
        "    parser.add_argument('--max_steps', default=100, type=int)\n",
        "\n",
        "    parser.add_argument('--valid_steps', default=20, type=int, help='how often to check accuracy on validation dataset')\n",
        "    parser.add_argument('--log_steps', default=10, type=int, help='train log every xx steps')\n",
        "    parser.add_argument('--test_log_steps', default=1000, type=int, help='valid/test log every xx steps')\n",
        "\n",
        "    parser.add_argument('--nentity', type=int, default=0, help='DO NOT MANUALLY SET')\n",
        "    parser.add_argument('--nrelation', type=int, default=0, help='DO NOT MANUALLY SET')\n",
        "\n",
        "    parser.parse_args(args)\n",
        "    return parser.parse_args(args)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TccTbD_UklT7"
      },
      "source": [
        "## Logging\n",
        "The below function is used to help with logging metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2a2v-Cpkmwu"
      },
      "outputs": [],
      "source": [
        "def log_metrics(mode, step, metrics):\n",
        "    '''\n",
        "    Print the evaluation logs\n",
        "    '''\n",
        "    for metric in metrics:\n",
        "        print('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQcVNKj38kol"
      },
      "source": [
        "## Main Program Loop\n",
        "We can finally bring everything we've done together into the main program loop. Please refer to the comments in the code to understand how it operates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_sON4K08mAV"
      },
      "outputs": [],
      "source": [
        "def main(args):\n",
        "    if (not args.do_train) and (not args.do_valid) and (not args.do_test):\n",
        "        raise ValueError('one of train/val/test mode must be chosen.')\n",
        "\n",
        "    # use CUDA if possible\n",
        "    args.cuda = torch.cuda.is_available()\n",
        "\n",
        "    # print dataset parameters\n",
        "    entity2id = dataset.training.entity_to_id\n",
        "    relation2id = dataset.training.relation_to_id\n",
        "\n",
        "    nentity = len(entity2id)\n",
        "    nrelation = len(relation2id)\n",
        "\n",
        "    args.nentity = nentity\n",
        "    args.nrelation = nrelation\n",
        "\n",
        "    print('Model: %s' % args.model)\n",
        "    print('#entity: %d' % nentity)\n",
        "    print('#relation: %d' % nrelation)\n",
        "\n",
        "    print('#train: %d' % len(train_triples))\n",
        "    print('#valid: %d' % len(valid_triples))\n",
        "    print('#test: %d' % len(test_triples))\n",
        "\n",
        "    # all true triples\n",
        "    all_true_triples = train_triples + valid_triples + test_triples\n",
        "\n",
        "    # create the model\n",
        "    kge_model = KGEModel(\n",
        "        model_name=args.model,\n",
        "        nentity=nentity,\n",
        "        nrelation=nrelation,\n",
        "        hidden_dim=args.hidden_dim,\n",
        "        gamma=args.gamma,\n",
        "        double_entity_embedding=args.double_entity_embedding,\n",
        "        double_relation_embedding=args.double_relation_embedding\n",
        "    )\n",
        "\n",
        "    # output the model params\n",
        "    print('\\nModel Parameter Configuration:')\n",
        "    for name, param in kge_model.named_parameters():\n",
        "        print('Parameter %s: %s, require_grad = %s' % (name, str(param.size()), str(param.requires_grad)))\n",
        "\n",
        "    if args.cuda:\n",
        "        kge_model = kge_model.cuda()\n",
        "\n",
        "    if args.do_train:\n",
        "        # set training dataloader iterator\n",
        "        train_dataloader = DataLoader(\n",
        "            TrainDataset(train_triples, nentity, nrelation, args.negative_sample_size),\n",
        "            batch_size=args.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=max(1, args.cpu_num//2),\n",
        "            collate_fn=TrainDataset.collate_fn\n",
        "        )\n",
        "        train_iterator = OneShotIterator(train_dataloader)\n",
        "\n",
        "        # set training configuration\n",
        "        optimizer = torch.optim.Adam(\n",
        "            filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
        "            lr=args.learning_rate\n",
        "        )\n",
        "\n",
        "    init_step = 1\n",
        "    step = init_step\n",
        "\n",
        "    # do an initial evaluation to see metrics for a random model\n",
        "    print('\\nEvaluating initial model on Valid Dataset...')\n",
        "    metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
        "    log_metrics('Valid', step, metrics)\n",
        "\n",
        "    # log training parameters\n",
        "    print('\\nStart Training...')\n",
        "    print('init_step = %d' % init_step)\n",
        "    print('batch_size = %d' % args.batch_size)\n",
        "    print('hidden_dim = %d' % args.hidden_dim)\n",
        "    print('gamma = %f' % args.gamma)\n",
        "\n",
        "    if args.do_train:\n",
        "        print('learning_rate = %f' % args.learning_rate)\n",
        "\n",
        "        training_logs = []\n",
        "\n",
        "        # training loop\n",
        "        for step in range(init_step, args.max_steps):\n",
        "            # perform a single train step\n",
        "            log = train_step(kge_model, optimizer, train_iterator, args)\n",
        "\n",
        "            # record the logs\n",
        "            training_logs.append(log)\n",
        "\n",
        "            # check if logs should be displayed\n",
        "            if step % args.log_steps == 0:\n",
        "                metrics = {}\n",
        "                for metric in training_logs[0].keys():\n",
        "                    metrics[metric] = sum([log[metric] for log in training_logs])/len(training_logs)\n",
        "                print('\\nTraining metrics...')\n",
        "                log_metrics('Training average', step, metrics)\n",
        "                training_logs = []\n",
        "\n",
        "            # check if metrics should be reported on the validation set\n",
        "            if args.do_valid and step % args.valid_steps == 0:\n",
        "                print('\\nEvaluating on Valid Dataset...')\n",
        "                metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
        "                log_metrics('Valid', step, metrics)\n",
        "\n",
        "    # compute final metrics after training is complete\n",
        "\n",
        "    if args.do_valid:\n",
        "        print('Evaluating on Valid Dataset...')\n",
        "        metrics = test_step(kge_model, valid_triples, all_true_triples, args)\n",
        "        log_metrics('Valid', step, metrics)\n",
        "\n",
        "    if args.do_test:\n",
        "        print('Evaluating on Test Dataset...')\n",
        "        metrics = test_step(kge_model, test_triples, all_true_triples, args)\n",
        "        log_metrics('Test', step, metrics)\n",
        "\n",
        "    if args.evaluate_train:\n",
        "        print('Evaluating on Training Dataset...')\n",
        "        metrics = test_step(kge_model, train_triples, all_true_triples, args)\n",
        "        log_metrics('Test', step, metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code can be used to run the main program loop. Model arguments can be adjusted by changing / adding / removing the arguments."
      ],
      "metadata": {
        "id": "jNOqo1cs6FcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    main(parse_args(['--do_train', '--do_valid', '--do_test',\n",
        "                     '--model', 'TransE',\n",
        "                     '--max_steps', '1000', '--valid_steps', '20', '--log_steps', '10']))"
      ],
      "metadata": {
        "id": "i-OAPHgf6Eqa"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}